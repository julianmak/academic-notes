\documentclass[letter-paper]{tufte-book}

%%
% Book metadata
\title{Linear Algebra 2H}
\author[]{B. S. H. Mithrandir}
%\publisher{Research Institute of Valinor}

%%
% If they're installed, use Bergamo and Chantilly from www.fontsite.com.
% They're clones of Bembo and Gill Sans, respectively.
\IfFileExists{bergamo.sty}{\usepackage[osf]{bergamo}}{}% Bembo
\IfFileExists{chantill.sty}{\usepackage{chantill}}{}% Gill Sans

%\usepackage{microtype}
\usepackage{amssymb}
\usepackage{amsmath}
%%
% For nicely typeset tabular material
\usepackage{booktabs}

%% overunder braces
\usepackage{oubraces}

%% 
\usepackage{xcolor}
\usepackage{tcolorbox}

\newtcolorbox[auto counter,number within=section]{derivbox}[2][]{colback=TealBlue!5!white,colframe=TealBlue,title=Box \thetcbcounter:\ #2,#1}                                                          

\makeatletter
\@openrightfalse
\makeatother

%%
% For graphics / images
\usepackage{graphicx}
\setkeys{Gin}{width=\linewidth,totalheight=\textheight,keepaspectratio}
\graphicspath{{figs/}}

% The fancyvrb package lets us customize the formatting of verbatim
% environments.  We use a slightly smaller font.
\usepackage{fancyvrb}
\fvset{fontsize=\normalsize}

\usepackage[plain]{fancyref}
\newcommand*{\fancyrefboxlabelprefix}{box}
\fancyrefaddcaptions{english}{%
  \providecommand*{\frefboxname}{Box}%
  \providecommand*{\Frefboxname}{Box}%
}
\frefformat{plain}{\fancyrefboxlabelprefix}{\frefboxname\fancyrefdefaultspacing#1}
\Frefformat{plain}{\fancyrefboxlabelprefix}{\Frefboxname\fancyrefdefaultspacing#1}

%%
% Prints argument within hanging parentheses (i.e., parentheses that take
% up no horizontal space).  Useful in tabular environments.
\newcommand{\hangp}[1]{\makebox[0pt][r]{(}#1\makebox[0pt][l]{)}}

%% 
% Prints an asterisk that takes up no horizontal space.
% Useful in tabular environments.
\newcommand{\hangstar}{\makebox[0pt][l]{*}}

%%
% Prints a trailing space in a smart way.
\usepackage{xspace}
\usepackage{xstring}

%%
% Some shortcuts for Tufte's book titles.  The lowercase commands will
% produce the initials of the book title in italics.  The all-caps commands
% will print out the full title of the book in italics.
\newcommand{\vdqi}{\textit{VDQI}\xspace}
\newcommand{\ei}{\textit{EI}\xspace}
\newcommand{\ve}{\textit{VE}\xspace}
\newcommand{\be}{\textit{BE}\xspace}
\newcommand{\VDQI}{\textit{The Visual Display of Quantitative Information}\xspace}
\newcommand{\EI}{\textit{Envisioning Information}\xspace}
\newcommand{\VE}{\textit{Visual Explanations}\xspace}
\newcommand{\BE}{\textit{Beautiful Evidence}\xspace}

\newcommand{\TL}{Tufte-\LaTeX\xspace}

% Prints the month name (e.g., January) and the year (e.g., 2008)
\newcommand{\monthyear}{%
  \ifcase\month\or January\or February\or March\or April\or May\or June\or
  July\or August\or September\or October\or November\or
  December\fi\space\number\year
}


\newcommand{\urlwhitespacereplace}[1]{\StrSubstitute{#1}{ }{_}[\wpLink]}

\newcommand{\wikipedialink}[1]{http://en.wikipedia.org/wiki/#1}% needs \wpLink now

\newcommand{\anonymouswikipedialink}[1]{\urlwhitespacereplace{#1}\href{\wikipedialink{\wpLink}}{Wikipedia}}

\newcommand{\Wikiref}[1]{\urlwhitespacereplace{#1}\href{\wikipedialink{\wpLink}}{#1}}

% Prints an epigraph and speaker in sans serif, all-caps type.
\newcommand{\openepigraph}[2]{%
  %\sffamily\fontsize{14}{16}\selectfont
  \begin{fullwidth}
  \sffamily\large
  \begin{doublespace}
  \noindent\allcaps{#1}\\% epigraph
  \noindent\allcaps{#2}% author
  \end{doublespace}
  \end{fullwidth}
}

% Inserts a blank page
\newcommand{\blankpage}{\newpage\hbox{}\thispagestyle{empty}\newpage}

\usepackage{units}

% Typesets the font size, leading, and measure in the form of 10/12x26 pc.
\newcommand{\measure}[3]{#1/#2$\times$\unit[#3]{pc}}

% Macros for typesetting the documentation
\newcommand{\hlred}[1]{\textcolor{Maroon}{#1}}% prints in red
\newcommand{\hangleft}[1]{\makebox[0pt][r]{#1}}
\newcommand{\hairsp}{\hspace{1pt}}% hair space
\newcommand{\hquad}{\hskip0.5em\relax}% half quad space
\newcommand{\TODO}{\textcolor{red}{\bf TODO!}\xspace}
\newcommand{\na}{\quad--}% used in tables for N/A cells
\providecommand{\XeLaTeX}{X\lower.5ex\hbox{\kern-0.15em\reflectbox{E}}\kern-0.1em\LaTeX}
\newcommand{\tXeLaTeX}{\XeLaTeX\index{XeLaTeX@\protect\XeLaTeX}}
% \index{\texttt{\textbackslash xyz}@\hangleft{\texttt{\textbackslash}}\texttt{xyz}}
\newcommand{\tuftebs}{\symbol{'134}}% a backslash in tt type in OT1/T1
\newcommand{\doccmdnoindex}[2][]{\texttt{\tuftebs#2}}% command name -- adds backslash automatically (and doesn't add cmd to the index)
\newcommand{\doccmddef}[2][]{%
  \hlred{\texttt{\tuftebs#2}}\label{cmd:#2}%
  \ifthenelse{\isempty{#1}}%
    {% add the command to the index
      \index{#2 command@\protect\hangleft{\texttt{\tuftebs}}\texttt{#2}}% command name
    }%
    {% add the command and package to the index
      \index{#2 command@\protect\hangleft{\texttt{\tuftebs}}\texttt{#2} (\texttt{#1} package)}% command name
      \index{#1 package@\texttt{#1} package}\index{packages!#1@\texttt{#1}}% package name
    }%
}% command name -- adds backslash automatically
\newcommand{\doccmd}[2][]{%
  \texttt{\tuftebs#2}%
  \ifthenelse{\isempty{#1}}%
    {% add the command to the index
      \index{#2 command@\protect\hangleft{\texttt{\tuftebs}}\texttt{#2}}% command name
    }%
    {% add the command and package to the index
      \index{#2 command@\protect\hangleft{\texttt{\tuftebs}}\texttt{#2} (\texttt{#1} package)}% command name
      \index{#1 package@\texttt{#1} package}\index{packages!#1@\texttt{#1}}% package name
    }%
}% command name -- adds backslash automatically
\newcommand{\docopt}[1]{\ensuremath{\langle}\textrm{\textit{#1}}\ensuremath{\rangle}}% optional command argument
\newcommand{\docarg}[1]{\textrm{\textit{#1}}}% (required) command argument
\newenvironment{docspec}{\begin{quotation}\ttfamily\parskip0pt\parindent0pt\ignorespaces}{\end{quotation}}% command specification environment
\newcommand{\docenv}[1]{\texttt{#1}\index{#1 environment@\texttt{#1} environment}\index{environments!#1@\texttt{#1}}}% environment name
\newcommand{\docenvdef}[1]{\hlred{\texttt{#1}}\label{env:#1}\index{#1 environment@\texttt{#1} environment}\index{environments!#1@\texttt{#1}}}% environment name
\newcommand{\docpkg}[1]{\texttt{#1}\index{#1 package@\texttt{#1} package}\index{packages!#1@\texttt{#1}}}% package name
\newcommand{\doccls}[1]{\texttt{#1}}% document class name
\newcommand{\docclsopt}[1]{\texttt{#1}\index{#1 class option@\texttt{#1} class option}\index{class options!#1@\texttt{#1}}}% document class option name
\newcommand{\docclsoptdef}[1]{\hlred{\texttt{#1}}\label{clsopt:#1}\index{#1 class option@\texttt{#1} class option}\index{class options!#1@\texttt{#1}}}% document class option name defined
\newcommand{\docmsg}[2]{\bigskip\begin{fullwidth}\noindent\ttfamily#1\end{fullwidth}\medskip\par\noindent#2}
\newcommand{\docfilehook}[2]{\texttt{#1}\index{file hooks!#2}\index{#1@\texttt{#1}}}
\newcommand{\doccounter}[1]{\texttt{#1}\index{#1 counter@\texttt{#1} counter}}

\newcommand{\studyq}[1]{\marginnote{Q: #1}}

\hypersetup{colorlinks}% uncomment this line if you prefer colored hyperlinks (e.g., for onscreen viewing)

% Generates the index
\usepackage{makeidx}
\makeindex

\setcounter{tocdepth}{3}
\setcounter{secnumdepth}{3}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% custom commands

\newtheorem{theorem}{\color{pastel-blue}Theorem}[section]
\newtheorem{lemma}[theorem]{\color{pastel-blue}Lemma}
\newtheorem{proposition}[theorem]{\color{pastel-blue}Proposition}
\newtheorem{corollary}[theorem]{\color{pastel-blue}Corollary}

\newenvironment{proof}[1][Proof]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{example}[1][Example]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

\hyphenpenalty=5000

% more pastel ones
\xdefinecolor{pastel-red}{rgb}{0.77,0.31,0.32}
\xdefinecolor{pastel-green}{rgb}{0.33,0.66,0.41}
\definecolor{pastel-blue}{rgb}{0.30,0.45,0.69} % crayola blue
\definecolor{gray}{rgb}{0.2,0.2,0.2} % dark gray

\xdefinecolor{orange}{rgb}{1,0.45,0}
\xdefinecolor{green}{rgb}{0,0.35,0}
\definecolor{blue}{rgb}{0.12,0.46,0.99} % crayola blue
\definecolor{gray}{rgb}{0.2,0.2,0.2} % dark gray

\xdefinecolor{cerulean}{rgb}{0.01,0.48,0.65}
\xdefinecolor{ust-blue}{rgb}{0,0.20,0.47}
\xdefinecolor{ust-mustard}{rgb}{0.67,0.52,0.13}

%\newcommand\comment[1]{{\color{red}#1}}

\newcommand{\dy}{\partial}
\newcommand{\ddy}[2]{\frac{\dy#1}{\dy#2}}

\newcommand{\ex}{\mathrm{e}}
\newcommand{\zi}{{\rm i}}

\newcommand\Real{\mbox{Re}} % cf plain TeX's \Re and Reynolds number
\newcommand\Imag{\mbox{Im}} % cf plain TeX's \Im

\newcommand{\zbar}{{\overline{z}}}

\newcommand{\inner}[2]{\langle #1, #2\rangle}

\newcommand{\As}{{\mathsf{A}}}
\newcommand{\Bs}{{\mathsf{B}}}
\newcommand{\Ds}{{\mathsf{D}}}
\newcommand{\Is}{{\mathsf{I}}}
\newcommand{\Ps}{{\mathsf{P}}}
\newcommand{\Xs}{{\mathsf{X}}}
\newcommand{\Ys}{{\mathsf{Y}}}
\newcommand{\Ob}{{\boldsymbol{0}}}
\newcommand{\eb}{{\boldsymbol{e}}}
\newcommand{\ub}{{\boldsymbol{u}}}
\newcommand{\vb}{{\boldsymbol{v}}}
\newcommand{\wb}{{\boldsymbol{w}}}
\newcommand{\xb}{{\boldsymbol{x}}}
\newcommand{\yb}{{\boldsymbol{y}}}
\newcommand{\zb}{{\boldsymbol{z}}}

\newcommand\Def[1]{\textbf{#1}}

\newcommand{\qed}{\hfill$\blacksquare$}
\newcommand{\qedwhite}{\hfill \ensuremath{\Box}}

\newcommand{\highlight}[1]{\mathchoice%
  {\colorbox{black!10}{$\displaystyle#1$}}%
  {\colorbox{black!10}{$\textstyle#1$}}%
  {\colorbox{black!10}{$\scriptstyle#1$}}%
  {\colorbox{black!10}{$\scriptscriptstyle#1$}}}%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% some extra formatting (hacked from Patrick Farrell's notes)
%  https://courses.maths.ox.ac.uk/node/view_material/4915
%

% chapter format
\titleformat{\chapter}%
  {\huge\rmfamily\itshape\color{pastel-red}}% format applied to label+text
  {\llap{\colorbox{pastel-red}{\parbox{1.5cm}{\hfill\itshape\huge\color{white}\thechapter}}}}% label
  {1em}% horizontal separation between label and title body
  {}% before the title body
  []% after the title body

% section format
\titleformat{\section}%
  {\normalfont\Large\itshape\color{pastel-green}}% format applied to label+text
  {\llap{\colorbox{pastel-green}{\parbox{1.5cm}{\hfill\color{white}\thesection}}}}% label
  {1em}% horizontal separation between label and title body
  {}% before the title body
  []% after the title body

% subsection format
\titleformat{\subsection}%
  {\normalfont\large\itshape\color{pastel-blue}}% format applied to label+text
  {\llap{\colorbox{pastel-blue}{\parbox{1.5cm}{\hfill\color{white}\thesubsection}}}}% label
  {1em}% horizontal separation between label and title body
  {}% before the title body
  []% after the title body

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

% Front matter
%\frontmatter

% r.3 full title page
%\maketitle

% v.4 copyright page

\chapter*{}

\begin{fullwidth}

\par \begin{center}{\Huge Linera Algebra 2H}\end{center}

\vspace*{5mm}

\par \begin{center}{\Large typed up by B. S. H. Mithrandir}\end{center}

\vspace*{5mm}

\begin{itemize}
  \item \textit{Last compiled: \monthyear}
  \item Blended from notes of C. Keaton and J. R. Parker, Durham
  \item This was part of the Durham core second year modules. Involves
  introduction to vector spaces, linear maps, and matrices as linear maps
  \item I have personally changed the ordering quite a bit, since the ordering
  in my original I have from the original notes is obscenely jumpy and makes no
  logical sense to me...(if I were teaching this I would probably have matrices
  before linear maps)
\end{itemize}

\par

\par Licensed under the Apache License, Version 2.0 (the ``License''); you may
not use this file except in compliance with the License. You may obtain a copy
of the License at \url{http://www.apache.org/licenses/LICENSE-2.0}. Unless
required by applicable law or agreed to in writing, software distributed under
the License is distributed on an \smallcaps{``AS IS'' BASIS, WITHOUT WARRANTIES
OR CONDITIONS OF ANY KIND}, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
\end{fullwidth}


%===============================================================================

\chapter{Vector spaces}

Recall that a \textbf{field} $\mathbb{F}$ equipped with an addition $+$ and a
multiplication $\times$ operation satisfies the following axioms:
\begin{itemize}
  \item operations are associative
  \item operations are commutative
  \item there exists additive and multiplicative identity
  \item there exists additive and multiplicative inverse (except for the
  additive identity)
  \item multiplication is distributive over addition
\end{itemize}

The usual rationals $\mathbb{Q}$, reals $\mathbb{R}$ and complex numbers
$\mathbb{C}$ are common fields.

A set $V$ is a \textbf{vector space} over a field $\mathbb{F}$ if, for $\vb, \ub \in V$ and $\lambda, \mu \in \mathbb{F}$,
\begin{itemize}
  \item $V$ form an abelian group under addition $+$
  \item for all $\lambda\vb \in V$
  \item $(\lambda + \mu)\vb = \lambda \vb + \mu \vb$
  \item $\lambda(\ub + \vb) = \lambda \ub + \lambda \vb$
  \item $\lambda (\mu \vb) = (\lambda \mu) \vb$
  \item there exists an identity element $1$ where $1\vb = \vb$.
\end{itemize}
Some examples are
\begin{itemize}
  \item $\mathbb{F} = \mathbb{R}$ and $V = \mathbb{C}$
  \item $\mathbb{F} = \mathbb{R}$ and $V = \mathbb{R}^n$ is the Eucliean $n$-space
  \item $\mathbb{F} = \mathbb{R}$ and $V = C[0, 1]$ the set of continuous
  functions defined over the interval $[0, 1]$
  \item $\mathbb{F} = \mathbb{R}$ and $V$ the solutions to the ODE $y'' - y' + y
  = 0$
  \item $\mathbb{F} = \mathbb{R}$ and $V = \mathbb{R}[t]$, the set of
  polynomials in $t$ with coefficients in $\mathbb{R}$
\end{itemize}

\begin{lemma}
  If $V$ is a vector space over $\mathbb{F}$, then for all $\vb \in V$, we have
  $0\vb = \boldsymbol{0}$.
\end{lemma}

\begin{proof}
  We have $0\vb = (0 + 0)\vb = 0\vb + 0\vb$, so subtracting accordingly leads to
  the above result. \qed
\end{proof}

\begin{lemma}
  As above, but for all $\lambda \in \mathbb{F}$, we have $\lambda
  \boldsymbol{0} = \boldsymbol{0}$. \qedwhite
\end{lemma}

\begin{lemma}
  For all $\vb\in V$, $(-1)\vb = -\vb$, the additive inverse of $\vb$.
\end{lemma}

\begin{proof}
  $\boldsymbol{0} = 0\vb = (1 + (-1))\vb = \vb + (-1)\vb$, so that $(-1)\vb =
  -\vb$. \qed
\end{proof}

Let $U \subseteq V$, then $U$ is a \textbf{subspace} of $V$ over $\mathbb{F}$ if $U$ forms a vector space under the same addition and scalar multiplication as $V$.

Instead of checking that a given subspace $U$ really satisfies all the vector
space axioms, the follow theorem reduces the amount of work required.

\begin{theorem}
  $U$ is a subspace of $V$ if and only if $U \neq \emptyset$, and for all $\ub,
  \vb \in U$ and $\lambda \in \mathbb{F}$, we have $\ub + \vb \in U$ and
  $\lambda \vb \in U$.
\end{theorem}

\begin{proof}
  These conditions are obviously necessary for $U$ to be a vector space.
  Conversely, if $\ub, \vb \in U$, then $\ub$, $-\vb = (-1)\vb \in U$ by above
  lemma.s Since $\ub - \vb \in U$, $U \neq \emptyset$, and $U$ is a group under
  addition. The condition $\lambda \vb in U$ ensures that $U$ is closed. Since
  the relevant axioms hold in $V$, they also hold in $U$. \qed
\end{proof}

\begin{corollary}
  Let $U$ and $V$ be subspaces of $W$, then $U \cap V$ is a subspace of $W$.
\end{corollary}

%-------------------------------------------------------------------------------

\section{Bases}

We say the vectors $\{\vb_i\}$ are \textbf{linearly independent} if
\begin{equation}
  \lambda_1 \vb_1 + \ldots + \lambda_n \vb_n = 0
\end{equation}
only has trivial solutions $\lambda_i = 0$ for all $i$, if $\vb_i \neq \vb_j$
for $i\neq j$.

(examples)

The \textbf{span} of $\{\vb_i\}$ is defines as the set containing all linear
combinations of the \textbf{spanning set} $\{\vb_i\}$, i.e.
\begin{equation}
  \mbox{Span}(\{\vb_i\}) = \left\{ \sum_{i=1}^n \lambda_i \vb_i \ |\ \lambda_i \in \mathbb{F} \right\}.
\end{equation}

The set $\{\vb_i\}$ is a \textbf{basis} of some vector space $U$ if the spanning
set is linearly independent, and spans $U$.

%-------------------------------------------------------------------------------

\section{Sums and direct sums}

%-------------------------------------------------------------------------------

\section{Inner products}

%-------------------------------------------------------------------------------

\section{Gram--Schmidt orthonomalisation}

%-------------------------------------------------------------------------------

\section{Orthogonal projection}

%===============================================================================

\chapter{Linear maps}

%-------------------------------------------------------------------------------

\section{Spaces of linear maps}

%-------------------------------------------------------------------------------

\section{Dual space}

%===============================================================================

\section{Matrices}

Mostly going to assume we are working with matrices defined over the reals,
unless stated otherwise.

%-------------------------------------------------------------------------------

\section{Elementary matrices}

%-------------------------------------------------------------------------------

\section{Rank}

Let $\As$ be a $m\times n$ matrix. Then the \textbf{row rank} of $\As$ is the
dimension of the subspaces spanned by the row vectors of $\As$. The
\textbf{column rank} is defined similarly.

\begin{theorem}
  For every matrix, the row rank is equal to the column rank (so we only need to
  talk about \emph{rank}).
\end{theorem}

\begin{proof}
  Think of $\As$ as a linear map $\theta: \mathbb{R}^n \to \mathbb{R}^m$, so
  there exists non-singular $\Xs$ and $\Ys$ where
  \begin{equation*}
    \Bs = \Xs \As \Ys = \begin{pmatrix}\Is_r & 0 \\ 0 & 0 \end{pmatrix},
  \end{equation*}
  where $r$ is the row rank of $\theta$. The transpose of $\Bs$ has the same
  rank, although that is now the column rank of $\Bs^T$. \qed
\end{proof}

We define the \textbf{nullity} of $\As$ to be the dimension of the kernel, i.e.,
the dimension of the solution set $\As\xb = \Ob$.

\begin{lemma}
  The following are equivalent if $\As$ is a square $n\times n$ matrix:
  \begin{enumerate}
    \item $\As$ is invertible,
    \item $\mbox{rank}(\As) = n$,
    \item $\mbox{null}(\As) = 0$.
  \end{enumerate}
\end{lemma}

\begin{proof}
  If $\As$ is invertible then the corresponding linear map $\theta$ is an
  isomorphism, and we have the result by a previous corollary.
  {\color{red}reference back} \qed
\end{proof}

%-------------------------------------------------------------------------------

\section{Trace and determinant}

%-------------------------------------------------------------------------------

\section{Adjugate}

%-------------------------------------------------------------------------------

\section{Eigenvalues and eigenvectors}

Let $V$ be a vector space, and $\theta:V \to V$ be a linear map. Then $\vb\in
V-\{\Ob\}$ is an \textbf{eigenvector} of $\theta$ if
\begin{equation}
  \highlight{\theta(\vb) = \lambda \vb},
\end{equation}
where $\lambda$ is the corresponding \textbf{eigenvalue}. The subset
\begin{equation}
  V_{\lambda} = \{\vb\in V \ |\ \theta(\vb) = \lambda \vb\}
\end{equation}
is a vector space called the \textbf{eigenspace} corresponding to the eigenvalue
$\lambda$.

Clearly if $\As$ is the corresponding matrix to $\theta$ then the eigenvalues
and eigenvectors would correspond accordingly.

Two $n\times n$ matrices $\As$ and $\Bs$ are called \textbf{similar} if there
exists some non-singular $\Ys$ where $\Bs = \Ys^{-1} \As \Ys$.

\begin{theorem}
  If $\As$ corresponds to the linear map $\theta:V\to V$, then the roots of the
  \textbf{characteristic polynomial} $\highlight{|\lambda\Is - \As| = 0}$ are
  the eigenvalues of $\theta$. This is independent of the basis of $V$.
\end{theorem}

\begin{proof}
  Let $\vb$ be an eigenvector, so that $\vb=\sum_i v_i\eb_i$. If $\lambda$ is
  the corresponding eigenvalue, and $\textsf{A}$ is the representation of
  $\theta$ in the basis $\{\eb_i\}$, then
  \begin{equation*}
    \As\vb = \lambda\vb \quad \Leftrightarrow \quad (\lambda\Is - \As)\vb = \Ob.
  \end{equation*}
  Since $\vb$ is non-trivial by definition of it being an eigenvector, then
  $\lambda\Is - \As$ is singular, so $|\lambda\Is - \As| = 0$.
  
  For basis independence, we know that
  \begin{align*}
    |\lambda\Is - \Ys^{-1}\As\Ys| &= |\lambda\Ys^{-1}\Is\Ys - \Ys^{-1}\As\Ys| \\
      & = |\Ys^{-1}(\lambda\Is- \As)\Ys|\\
      & = |\Ys^{-1}| \cdot |\lambda\Is- \As| \cdot |\Ys|,
  \end{align*}
  but by the property that $|\Ys^{-1}| \cdot |\Ys| = 1$, we have $|\lambda\Is -
  \Ys^{-1}\As\Ys| = |\lambda\Is- \As| = 0$. \qed
\end{proof}

\begin{example}
  For
  \begin{equation*}
    \As = \begin{pmatrix}2 & 2 \\ 0 & 4\end{pmatrix}, \quad \lambda\Is - \As = \begin{pmatrix}\lambda - 2 & -2 \\ 0 & \lambda - 4\end{pmatrix}
  \end{equation*}
  and the eigenvalues satisfy\marginnote{Could have done this with the trace and
  determinant also for a $2\times2$ matrix.} the quadratic
  $(\lambda-2)(\lambda-4) = 0$, so $\lambda=2, 4$. For $\lambda=2$, the
  eigenvector satisfies
  \begin{equation*}
    \begin{pmatrix}0 & -2 \\ 0 & -2\end{pmatrix}\begin{pmatrix}v_1 \\ v_2 \end{pmatrix} = \Ob,
  \end{equation*}
  which implies $v_2 = 0$ but $v_1$ is undetermined; we could choose for
  simplicity for the eigenvector to be
  \begin{equation*}
   \vb_{\lambda=2} = \begin{pmatrix}1 \\ 0 \end{pmatrix}.
  \end{equation*}
  For $\lambda = 4$, we result in $v_1 = v_2$, so we could choose
  \begin{equation*}
   \vb_{\lambda=4} = \begin{pmatrix}1 \\ 1 \end{pmatrix}.
  \end{equation*}
\end{example}

Notice for the above that if we take $\Ys$ to be the matrix whose column vectors
are the eigenvectors, i.e.
\begin{equation*}
  \Ys = \begin{pmatrix}1 & 0 \\ 1 & 1\end{pmatrix} \quad \Rightarrow \quad \Ys^{-1} = \begin{pmatrix}1 & -1 \\ 0 & 1\end{pmatrix},
\end{equation*}
then notice that 
\begin{equation*}
  \Ys^{-1} \As \Ys = \begin{pmatrix}1 & 0 \\ 1 & 1\end{pmatrix}\begin{pmatrix}2 & 2 \\ 0 & 4\end{pmatrix}\begin{pmatrix}1 & -1 \\ 0 & 1\end{pmatrix} = \begin{pmatrix}2 & 0 \\ 0 & 4\end{pmatrix},
\end{equation*}
which is just the \textbf{diagonal} matrix with the eigenvalues on the main
diagonal. This is called \textbf{diagonalisation}.

\begin{theorem}
  Let $\As$ be a $n\times n$ matrix. The matrix $\As$ is \textbf{diagonalisable}
  if there exists a non-singular $\Ys$ such that $\As$ is similar to a diagonal
  matrix $\Ds$, i.e. $\highlight{\Ds = \Ys^{-1} \As \Ys}$ or $\highlight{\As =
  \Ys \Ds\Ys^{-1}}$. This is equivalent to the condition that the eigenvectors
  of $\As$ has full span.
\end{theorem}

Notice we do not require $\As$ to be non-singular, nor that the eigenvalues have
to be distinct. For example,
\begin{equation*}
  \begin{pmatrix}1 & 0 \\ 0 & 0\end{pmatrix}, \quad \begin{pmatrix}1 & 0 \\ 0 & 1\end{pmatrix}
\end{equation*}
are respectively singular and have repeated eigenvalues, but are both trivially
diagonal (so diagonalisable by $\Is$). On the other hand,
\begin{equation*}
  \begin{pmatrix}0 & 1 \\ 1 & 0\end{pmatrix}
\end{equation*}
has eigenvalues $\lambda = \pm \zi$, so it is definitely not diagonalisable
\emph{over the reals}.

\begin{example}
  For
  \begin{equation*}
    \As = \begin{pmatrix}3 & 0 \\ 1 & 3\end{pmatrix},
  \end{equation*}
  we have $\lambda=3$ of multiplicity 2, but additionally only one eigenvector
  $(1,0)^T$, so the span of the eigenspace is only dimension 1, and matrix is
  not diagonalisable.
\end{example}

\begin{example}
  For
  \begin{equation*}
    \As = \begin{pmatrix}0 & -2 & 2 \\ 0 & 2 & 0 \\ -1 & -1 & 3\end{pmatrix},
  \end{equation*}
  can show that the eigenvalues are $\lambda=1$ and $\lambda=2$ twice. For
  eigenvector corresponding to $\lambda=1$ can be chosen to be
  \begin{equation*}
    \vb_1 = \begin{pmatrix}2 \\ 0 \\ 1\end{pmatrix}.
  \end{equation*}
  For $\lambda=2$, we reduce down to $v_1 + v_2 - v_3 = 0$, so we could take
  \begin{equation*}
    \vb_2 = \begin{pmatrix}1 \\ 0 \\ 1\end{pmatrix}, \quad \vb_3 = \begin{pmatrix}0 \\ 1 \\ 1\end{pmatrix},
  \end{equation*}
  which are linearly independent of each other, so the eigenspace has full span,
  and the transformation matrix could be
  \begin{equation*}
    \Ys = \begin{pmatrix}2 & 0 & 1 \\ 1 & 0 & 1 \\ 0 & 1 & 1\end{pmatrix}.
  \end{equation*}
  Notice we could have made a different choice for $\vb_{2,3}$, and
  that
  \begin{equation*}
    \Ys = \begin{pmatrix}2 & 0 & 1 \\ 1 & 0 & 1 \\ 2 & -1 & 1\end{pmatrix}
  \end{equation*}
  would have also worked.
\end{example}

There are at least two notable uses for this:
\begin{example}
  \begin{enumerate}
    \item If we have the system of ODEs
    \begin{equation*}
      \frac{\mathrm{d}}{\mathrm{d}t}\xb = \As\xb
    \end{equation*}
    where $\As$ is constant and diagonalisable with $\As = \Ys \Ds \Ys^{-1}$,
    then the solutions are
    \begin{equation*}
      \xb = \Ys\begin{pmatrix}\ex^{\lambda_1 t} \\ \vdots \\ \ex^{\lambda_n t}\end{pmatrix}
    \end{equation*}
    up to multiplicative constants (which are specified by the initial
    conditions). This is because
    \begin{equation*}
      \frac{\mathrm{d}}{\mathrm{d}t}\xb = \Ys \Ds \Ys^{-1}\xb \quad \Rightarrow \frac{\mathrm{d}}{\mathrm{d}t} \Ys^{-1}\xb = \Ds\Ys^{-1}\xb,
    \end{equation*}
    because $\Ys$ is constant in time so commutes with the derivative. Then
    calling $\yb = \Ys^{-1}\xb$, we just have a set of trivial constant ODEs to
    solve, then we invert to get $\xb = \Ys\yb$ which is known. 
    
    In other words, we choose a convenient basis where our problem is diagonal,
    solve the problem there (which is easy), and then transform it back into the
    original basis. This kind of problem occurs occasionally in quantum
    mechanics, where you might want to solve the \textbf{time-independent
    Schr\"odinger equation}
    \begin{equation*}
      \zi\hbar\frac{\mathrm{d}}{\mathrm{d}t}|\psi(t)\rangle = \mathsf{H}|\psi(t)\rangle
    \end{equation*}
    where $\mathsf{H}$ would be matrix representation of the relevant
    Hamiltonian of the system\marginnote{The Hamiltonian normally has additional
    properties that make the eigenvalues and eigenvectors have certain
    properties; more later when we talk about
    \emph{Hermitian}/\emph{self-adjoint} matrices.}, and $|\psi(t)\rangle$ would
    be the vector representation of the \textbf{state}. The eigenvalues
    correspond usually to an energy, and the eigenvectors the energy states.
    
    \item Suppose we have some system that is governed by
    \begin{equation*}
      \xb_{n+1} = \As\xb_n,
    \end{equation*}
    such as a \textbf{Markov chain} with discrete states, and so $\As$ is
    a matrix detailing the transition probabilities between states (with all
    positive entries that are strictly less than 1, since these are supposed to
    describe probabilities). If $\As$ is diagonalisable, then the
    numerics becomes `easy', in that to find $\xb_{n+1}$ from initial
    data $\xb_0$, we do
    \begin{align*}
      \xb_n &= \As^n\xb_0 \\ 
        &= (\Ys \Ds \Ys^{-1})^n \xb_0 \\
        &= (\Ys \Ds \Ys^{-1})(\Ys \Ds \Ys^{-1})\cdots(\Ys \Ds \Ys^{-1}) \xb_0 \\
        &= \Ys \Ds (\Ys^{-1}\Ys)\Ds\cdots(\Ys^{-1}\Ys)\Ds\Ys^{-1} \xb_0 \\
        &= \Ys \Ds^n \Ys^{-1} \xb_0,
    \end{align*}
    by associativity, and noting that $D^n$ is essentially trivial to
    compute.\marginnote{In the context of a Markov chain, the transition matrix
    have eigenvalues with magnitude less than or equal to 1 by the
    \emph{Perron--Frobenius theorem}, so we have no blow up. This is not
    necessarily true for population models such as the \emph{Leslie model},
    since the entries are positive but do not have to be strictly less than 1.}
  \end{enumerate}
\end{example}

%-------------------------------------------------------------------------------

\section{Cayley--Hamilton theorem}

%-------------------------------------------------------------------------------

\section{Orthogonal matrices}

Recall that two vectors are orthogonal to each other if $\ub\cdot\vb = \ub^T\vb
= 0$, the latter interpreted in terms of a matrix multiplication of a row vector
with a column vector. 

Analogously, a matrix is called \textbf{orthogonal} if the columns are
orthogonal to each other. However, a more useful form of orthogonality might be
the following:
\begin{theorem}
  A matrix $\As$ is orthogonal if and only if $\As^{-1} = \As^T$.\marginnote{So
  orthogonal matrices are great, because a transpose is much easier to compute
  than the inverse.}
\end{theorem}

\begin{proof}
  Let $\eb_i$ be the $i^{\textnormal{th}}$ column of $\Ps$, then $\As \As^T =
  (\eb_i^T \eb_j) = (\delta_{ij}) = \Is$, as required. \qed
\end{proof}

%-------------------------------------------------------------------------------

\section{Symmetric matrices}

A matrix is $\As$ is \textbf{symmetric} if $\highlight{\As = \As^T}$.

\begin{theorem}\label{thm:real_sym_eigen}
  Let $\As$ be a real symmetric matrix. Then the eigenvalues are all real, and
  the eigenvectors are mutual orthogonal to each other. \qedwhite
\end{theorem}

\begin{theorem}
  Let $\As$ be a real symmetric matrix. If it is diagonalisable, then the
  similarity matrix is orthogonal, i.e. $\As = \Ps \Ds \Ps^{-1} = \Ps \Ds
  \Ps^T$. \qedwhite
\end{theorem}

The theorems are special cases of those given in \S\ref{sec:hermitian}, so we
defer the proof for later.

\begin{example}
  Diagonalise
  \begin{equation*}
    \As = \begin{pmatrix}-1 & 1 & -1 \\ 1 & -1 & -1 \\ -1 & -1 & -1\end{pmatrix}.
  \end{equation*}
  
  Can show the eigenvalues are $1$, $-2$ and $-2$, and that
  \begin{equation*}
    \Ps = \begin{pmatrix}1 & 1 & 1 \\ 1 & 0 & -2 \\ -1 & 1 & 1\end{pmatrix}
  \end{equation*}
  is one possible choice, leading to $\Ds = \mbox{diag}(1, -2, -2)$.
\end{example}

%-------------------------------------------------------------------------------

\section{Rotations}

Denote the set of $n\times n$ orthogonal matrices over $\mathbb{R}$ by
$\mbox{O}(n)$, and $\mbox{SO}(n)$ the subset of $\mbox{O}(n)$ where the
corresponding matrices have determinant $+1$. These turn out to form a group
under matrix multiplication, and are known as the \textbf{orthogonal} and
\textbf{special orthogonal} groups of dimension $n$
respectively.\marginnote{These are actually \textbf{Lie groups}, which play an
important role in mathematical physics.}

\begin{theorem}
  Every element of $\mbox{SO}(3)$ represent a rotation about an arbitrary axis
  in $\mathbb{R}^3$. \qedwhite
\end{theorem}

\begin{corollary}
  The characteristic polynomial
  \begin{equation*}
    \Delta(\lambda) = \lambda^3 - (1+2\cos\theta)\lambda^2 + (1+2\cos\theta)\lambda - 1
  \end{equation*}
  tells us the angle of rotation $\theta$ (via $\cos\theta$, up to a sign) of a
  rotation about any axis.
\end{corollary}

\begin{example}
  Find the matrix which represents a rotation in $\mathbb{R}^3$ about
  $(1,1,0)^T$ through an angle of $\pi/2$.
  
  Note that we have
  \begin{align*}
    \mathsf{R}_x(\theta) = \begin{pmatrix}1 & 0 & 0 \\ 0 & \cos\theta & -\sin\theta \\ 0 & \sin\theta & \cos\theta\end{pmatrix}&, \quad
    \mathsf{R}_y(\theta) = \begin{pmatrix}\cos\theta & 0 & \sin\theta \\ 0 & 1 & 0 \\ -\sin\theta & 0 & \cos\theta\end{pmatrix}, \\
    \mathsf{R}_z(\theta) = &\begin{pmatrix}\cos\theta & -\sin\theta & 0 \\ \sin\theta & \cos\theta & 0 \\ 0 & 0 & 1\end{pmatrix}
  \end{align*}
  are the rotation matrices about the $x$, $y$ and $z$ axis respectively by an
  angle $\theta$. Then idea is that the axis is represented by a vector that we
  can normlalise, and we rotate that onto a standard axis (essentially a
  co-ordinate transformation), do the rotation by the desired angle (which we
  know the matrix of), and then undo rotation (or the co-ordinate
  transformation).
  
  We can take the $z$ axis for definiteness, and we want $\Ps:
  (1,1,0)^T/\sqrt{2} \mapsto (0,0,1)^T$. This is supposed to be a rotation, so
  $\Ps^{-1}$ exists and $\Ps^{-1}:(0,0,1)^T \mapsto (1,1,0)^T/\sqrt{2}$, or
  \begin{equation*}
    \Ps^{-1} = \begin{pmatrix}\cdot & \cdot & 1/\sqrt{2} \\ \cdot & \cdot & 1/\sqrt{2} \\ \cdot & \cdot & 0\end{pmatrix}.
  \end{equation*}
  The column vectors should be mutually orthogonal to each other, and we can
  choose them to be orthonormal, and we could consider the simple choice
  \begin{equation*}
    \Ps^{-1} \stackrel{!?}{=} \begin{pmatrix}0 & -1/\sqrt{2} & 1/\sqrt{2} \\ 0 & 1/\sqrt{2} & 1/\sqrt{2} \\ 1 & 0 & 0\end{pmatrix}.
  \end{equation*}
  Notice however that $|\Ps^{-1}|=-1$, so to make sure $\Ps\in\mbox{SO}(3)$, we
  can simply multiply the bottom row by $-1$. The resulting $\Ps$ is then
  \begin{equation*}
    \Ps = \begin{pmatrix}0 & 1/\sqrt{2} & 1/\sqrt{2} \\ 0 & -1/\sqrt{2} & 1/\sqrt{2} \\ 1 & 0 & 0\end{pmatrix},
  \end{equation*}
  and so
  \begin{equation*}
    \As = \Ps^{-1} \mathsf{R}_z(\pi/2) \Ps = \begin{pmatrix}1/2 & 1/2 & 1/\sqrt{2} \\ 1/2 & 1/2 & -1/\sqrt{2} \\ -1/\sqrt{2} & 1/\sqrt{2} & 0\end{pmatrix}
  \end{equation*}
  will do the job. Can brute force check that that $|\As|=1$, $\As^T = \As$, and
  the only solution to $\As\xb = \xb$ is $\xb = a(1, 1, 0)^T$.\marginnote{The
  rotation axis is kept fixed by the rotation about that axis.}
\end{example}

%-------------------------------------------------------------------------------

\section{Hermitian matrices}\label{sec:hermitian}

{\color{red}remember to introduce somewhere star being the Hermitian transpose}

A matrix over $\mathbb{C}$ is \textbf{Hermitian} if $\highlight{\As = \As^* =
\overline{\As}^T}$. Note that if $\As$ is actually real, then this reduces to
the definition of a symmetric matrix.

A matrix over $\mathbb{C}$ is \textbf{unitary} if $\highlight{\As^* =
\As^{-1}}$.\marginnote{Unitary matrixes are used a lot in quantum mechanics.}
Again, if $\As$ is actually real, then this reduces to the definition of an
orthogonal matrix.

\begin{theorem}
  Let $\As$ be a Hermitian matrix. Then the eigenvalues are all real, and the
  eigenvectors are mutual orthogonal to each other.\marginnote{Compare this with
  Theorem \ref{thm:real_sym_eigen}. In quantum mechanics the Hamiltonians are
  usually (but not always) chosen to be Hermitian, so the eigenvalues are all
  real and observable.}
\end{theorem}

\begin{proof}
  Let $\lambda$ be an eigenvalue of $\As$, so $\As\vb=\lambda\vb$ with
  $\vb\neq\Ob$. Now,
  \begin{equation*}
    \lambda\|\vb\|^2 = \lambda\vb^*\vb = \vb^* (\lambda\vb) = \vb^* \As\vb = \vb^* \As^*\vb = (\As\vb)^* \vb = \overline{\lambda}\vb^*\vb = \overline{\lambda}\|\vb\|^2,
  \end{equation*}
  so $\lambda$ is real for non-zero $\vb$.
  
  Let $\lambda$ and $\mu$ be distinct eigenvalues of $\As$, so there exists
  associated eigenvectors $\vb$ and $\wb$ respectively. Then doing similar
  manipulations to above,
  \begin{equation*}
    \lambda\wb^*\vb = \wb^* \As\vb = (\As\wb)^* \vb = \mu\wb^*\vb.
  \end{equation*}
  Since $\lambda\neq\mu$, then $\wb^*\vb = \inner{\vb}{\wb} = 0$, and so
  the eigenvectors are orthogonal. \qed
\end{proof}

\begin{theorem}
  Let $\As$ be a Hermitian matrix. If it is diagonalisable, then the similarity
  matrix is unitary, i.e. $\As = \Ps \Ds \Ps^{-1} = \Ps \Ds \Ps^*$.
\end{theorem}

\begin{proof}
  Let $\lambda_1$ be an eigenvalue of $\As$ with eigenvector $\vb$. Let $U =
  \mbox{span}\{\vb\}$. For $\wb \in U^\perp$, then
  \begin{equation*}
    \inner{\vb}{\As \wb} = \inner{\As \vb}{\wb} = \inner{\lambda_1\vb}{\wb} = \lambda_1\inner{\vb}{\wb} = 0,
  \end{equation*}
  hence $\As \wb \in U^\perp$. Supposing $\|\vb\|=1$, we choose an orthonormal
  basis $\ub_2, \ldots \ub_n$ for $U^\perp$, which implies that $\{\vb, \ub_2,
  \ldots \ub_n\}$ is an orthonormal basis for $\mathbb{C}^n$. 
  
  Let $\Ps_1$ be the matrix whose columns are those vectors, which by
  construction means $\Ps^* \Ps = \Is$ by orthogonality. Let $\{\eb_i\}$ be the
  standard basis of $\mathbb{C}^n$, then
  \begin{equation*}
    \Ps_1^* \As \Ps_1 \eb_1 = \Ps_1^* \As \vb = \Ps_1^* \lambda_1 \vb = \lambda_1 \Ps_1^* \vb = \lambda_1 \eb_1 \quad \Rightarrow \quad \Ps_1^* \As \Ps_1 = \begin{pmatrix}\lambda_1 & 0 \\ 0 & \As_1\end{pmatrix}.
  \end{equation*}
  The procedure can be continued for the next one, so that
  \begin{equation*}
    \Ps_2^* \Ps_1^* \As \Ps_1 \Ps_2 = \begin{pmatrix}\lambda_1 & 0 & 0 \\ 0 & \lambda_2 & 0 \\ 0 & 0 & \As_2\end{pmatrix},
  \end{equation*}
  and so we can define $\mathsf{P} = \prod_{i=1}^n \Ps_i$ as the transformation
  matrix. Since each $\Ps_i$ is unitary, its product is unitary. \qed
\end{proof}

%-------------------------------------------------------------------------------

\section{Diagonalising quadratic forms}

%-------------------------------------------------------------------------------

\section{Symmetric and self-adjoint maps}

%-------------------------------------------------------------------------------

\section{Reflections}

%-------------------------------------------------------------------------------

\section{The operator $\mathrm{d}/\mathrm{d}x^2$}

%-------------------------------------------------------------------------------

\section{Jordan normal form}

%===============================================================================

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% r.5 contents
%\tableofcontents

%\listoffigures

%\listoftables

% r.7 dedication
%\cleardoublepage
%~\vfill
%\begin{doublespace}
%\noindent\fontsize{18}{22}\selectfont\itshape
%\nohyphenation
%Dedicated to those who appreciate \LaTeX{} 
%and the work of \mbox{Edward R.~Tufte} 
%and \mbox{Donald E.~Knuth}.
%\end{doublespace}
%\vfill

% r.9 introduction
% \cleardoublepage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% actual useful crap (normal chapters)
\mainmatter

%\part{Basics (?)}


%\backmatter

%\bibliography{refs}
\bibliographystyle{plainnat}

%\printindex

\end{document}

