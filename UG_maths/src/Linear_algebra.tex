\documentclass[letter-paper]{tufte-book}

%%
% Book metadata
\title{Linear Algebra 2H}
\author[]{B. S. H. Mithrandir}
%\publisher{Research Institute of Valinor}

%%
% If they're installed, use Bergamo and Chantilly from www.fontsite.com.
% They're clones of Bembo and Gill Sans, respectively.
\IfFileExists{bergamo.sty}{\usepackage[osf]{bergamo}}{}% Bembo
\IfFileExists{chantill.sty}{\usepackage{chantill}}{}% Gill Sans

%\usepackage{microtype}
\usepackage{amssymb}
\usepackage{amsmath}
%%
% For nicely typeset tabular material
\usepackage{booktabs}

%% overunder braces
\usepackage{oubraces}

%% 
\usepackage{xcolor}
\usepackage{tcolorbox}

\newtcolorbox[auto counter,number within=section]{derivbox}[2][]{colback=TealBlue!5!white,colframe=TealBlue,title=Box \thetcbcounter:\ #2,#1}                                                          

\makeatletter
\@openrightfalse
\makeatother

%%
% For graphics / images
\usepackage{graphicx}
\setkeys{Gin}{width=\linewidth,totalheight=\textheight,keepaspectratio}
\graphicspath{{figs/}}

% The fancyvrb package lets us customize the formatting of verbatim
% environments.  We use a slightly smaller font.
\usepackage{fancyvrb}
\fvset{fontsize=\normalsize}

\usepackage[plain]{fancyref}
\newcommand*{\fancyrefboxlabelprefix}{box}
\fancyrefaddcaptions{english}{%
  \providecommand*{\frefboxname}{Box}%
  \providecommand*{\Frefboxname}{Box}%
}
\frefformat{plain}{\fancyrefboxlabelprefix}{\frefboxname\fancyrefdefaultspacing#1}
\Frefformat{plain}{\fancyrefboxlabelprefix}{\Frefboxname\fancyrefdefaultspacing#1}

%%
% Prints argument within hanging parentheses (i.e., parentheses that take
% up no horizontal space).  Useful in tabular environments.
\newcommand{\hangp}[1]{\makebox[0pt][r]{(}#1\makebox[0pt][l]{)}}

%% 
% Prints an asterisk that takes up no horizontal space.
% Useful in tabular environments.
\newcommand{\hangstar}{\makebox[0pt][l]{*}}

%%
% Prints a trailing space in a smart way.
\usepackage{xspace}
\usepackage{xstring}

%%
% Some shortcuts for Tufte's book titles.  The lowercase commands will
% produce the initials of the book title in italics.  The all-caps commands
% will print out the full title of the book in italics.
\newcommand{\vdqi}{\textit{VDQI}\xspace}
\newcommand{\ei}{\textit{EI}\xspace}
\newcommand{\ve}{\textit{VE}\xspace}
\newcommand{\be}{\textit{BE}\xspace}
\newcommand{\VDQI}{\textit{The Visual Display of Quantitative Information}\xspace}
\newcommand{\EI}{\textit{Envisioning Information}\xspace}
\newcommand{\VE}{\textit{Visual Explanations}\xspace}
\newcommand{\BE}{\textit{Beautiful Evidence}\xspace}

\newcommand{\TL}{Tufte-\LaTeX\xspace}

% Prints the month name (e.g., January) and the year (e.g., 2008)
\newcommand{\monthyear}{%
  \ifcase\month\or January\or February\or March\or April\or May\or June\or
  July\or August\or September\or October\or November\or
  December\fi\space\number\year
}


\newcommand{\urlwhitespacereplace}[1]{\StrSubstitute{#1}{ }{_}[\wpLink]}

\newcommand{\wikipedialink}[1]{http://en.wikipedia.org/wiki/#1}% needs \wpLink now

\newcommand{\anonymouswikipedialink}[1]{\urlwhitespacereplace{#1}\href{\wikipedialink{\wpLink}}{Wikipedia}}

\newcommand{\Wikiref}[1]{\urlwhitespacereplace{#1}\href{\wikipedialink{\wpLink}}{#1}}

% Prints an epigraph and speaker in sans serif, all-caps type.
\newcommand{\openepigraph}[2]{%
  %\sffamily\fontsize{14}{16}\selectfont
  \begin{fullwidth}
  \sffamily\large
  \begin{doublespace}
  \noindent\allcaps{#1}\\% epigraph
  \noindent\allcaps{#2}% author
  \end{doublespace}
  \end{fullwidth}
}

% Inserts a blank page
\newcommand{\blankpage}{\newpage\hbox{}\thispagestyle{empty}\newpage}

\usepackage{units}

% Typesets the font size, leading, and measure in the form of 10/12x26 pc.
\newcommand{\measure}[3]{#1/#2$\times$\unit[#3]{pc}}

% Macros for typesetting the documentation
\newcommand{\hlred}[1]{\textcolor{Maroon}{#1}}% prints in red
\newcommand{\hangleft}[1]{\makebox[0pt][r]{#1}}
\newcommand{\hairsp}{\hspace{1pt}}% hair space
\newcommand{\hquad}{\hskip0.5em\relax}% half quad space
\newcommand{\TODO}{\textcolor{red}{\bf TODO!}\xspace}
\newcommand{\na}{\quad--}% used in tables for N/A cells
\providecommand{\XeLaTeX}{X\lower.5ex\hbox{\kern-0.15em\reflectbox{E}}\kern-0.1em\LaTeX}
\newcommand{\tXeLaTeX}{\XeLaTeX\index{XeLaTeX@\protect\XeLaTeX}}
% \index{\texttt{\textbackslash xyz}@\hangleft{\texttt{\textbackslash}}\texttt{xyz}}
\newcommand{\tuftebs}{\symbol{'134}}% a backslash in tt type in OT1/T1
\newcommand{\doccmdnoindex}[2][]{\texttt{\tuftebs#2}}% command name -- adds backslash automatically (and doesn't add cmd to the index)
\newcommand{\doccmddef}[2][]{%
  \hlred{\texttt{\tuftebs#2}}\label{cmd:#2}%
  \ifthenelse{\isempty{#1}}%
    {% add the command to the index
      \index{#2 command@\protect\hangleft{\texttt{\tuftebs}}\texttt{#2}}% command name
    }%
    {% add the command and package to the index
      \index{#2 command@\protect\hangleft{\texttt{\tuftebs}}\texttt{#2} (\texttt{#1} package)}% command name
      \index{#1 package@\texttt{#1} package}\index{packages!#1@\texttt{#1}}% package name
    }%
}% command name -- adds backslash automatically
\newcommand{\doccmd}[2][]{%
  \texttt{\tuftebs#2}%
  \ifthenelse{\isempty{#1}}%
    {% add the command to the index
      \index{#2 command@\protect\hangleft{\texttt{\tuftebs}}\texttt{#2}}% command name
    }%
    {% add the command and package to the index
      \index{#2 command@\protect\hangleft{\texttt{\tuftebs}}\texttt{#2} (\texttt{#1} package)}% command name
      \index{#1 package@\texttt{#1} package}\index{packages!#1@\texttt{#1}}% package name
    }%
}% command name -- adds backslash automatically
\newcommand{\docopt}[1]{\ensuremath{\langle}\textrm{\textit{#1}}\ensuremath{\rangle}}% optional command argument
\newcommand{\docarg}[1]{\textrm{\textit{#1}}}% (required) command argument
\newenvironment{docspec}{\begin{quotation}\ttfamily\parskip0pt\parindent0pt\ignorespaces}{\end{quotation}}% command specification environment
\newcommand{\docenv}[1]{\texttt{#1}\index{#1 environment@\texttt{#1} environment}\index{environments!#1@\texttt{#1}}}% environment name
\newcommand{\docenvdef}[1]{\hlred{\texttt{#1}}\label{env:#1}\index{#1 environment@\texttt{#1} environment}\index{environments!#1@\texttt{#1}}}% environment name
\newcommand{\docpkg}[1]{\texttt{#1}\index{#1 package@\texttt{#1} package}\index{packages!#1@\texttt{#1}}}% package name
\newcommand{\doccls}[1]{\texttt{#1}}% document class name
\newcommand{\docclsopt}[1]{\texttt{#1}\index{#1 class option@\texttt{#1} class option}\index{class options!#1@\texttt{#1}}}% document class option name
\newcommand{\docclsoptdef}[1]{\hlred{\texttt{#1}}\label{clsopt:#1}\index{#1 class option@\texttt{#1} class option}\index{class options!#1@\texttt{#1}}}% document class option name defined
\newcommand{\docmsg}[2]{\bigskip\begin{fullwidth}\noindent\ttfamily#1\end{fullwidth}\medskip\par\noindent#2}
\newcommand{\docfilehook}[2]{\texttt{#1}\index{file hooks!#2}\index{#1@\texttt{#1}}}
\newcommand{\doccounter}[1]{\texttt{#1}\index{#1 counter@\texttt{#1} counter}}

\newcommand{\studyq}[1]{\marginnote{Q: #1}}

\hypersetup{colorlinks}% uncomment this line if you prefer colored hyperlinks (e.g., for onscreen viewing)

% Generates the index
\usepackage{makeidx}
\makeindex

\setcounter{tocdepth}{3}
\setcounter{secnumdepth}{3}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% custom commands

\newtheorem{theorem}{\color{pastel-blue}Theorem}[section]
\newtheorem{lemma}[theorem]{\color{pastel-blue}Lemma}
\newtheorem{proposition}[theorem]{\color{pastel-blue}Proposition}
\newtheorem{corollary}[theorem]{\color{pastel-blue}Corollary}

\newenvironment{proof}[1][Proof]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{example}[1][Example]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

\hyphenpenalty=5000

% more pastel ones
\xdefinecolor{pastel-red}{rgb}{0.77,0.31,0.32}
\xdefinecolor{pastel-green}{rgb}{0.33,0.66,0.41}
\definecolor{pastel-blue}{rgb}{0.30,0.45,0.69} % crayola blue
\definecolor{gray}{rgb}{0.2,0.2,0.2} % dark gray

\xdefinecolor{orange}{rgb}{1,0.45,0}
\xdefinecolor{green}{rgb}{0,0.35,0}
\definecolor{blue}{rgb}{0.12,0.46,0.99} % crayola blue
\definecolor{gray}{rgb}{0.2,0.2,0.2} % dark gray

\xdefinecolor{cerulean}{rgb}{0.01,0.48,0.65}
\xdefinecolor{ust-blue}{rgb}{0,0.20,0.47}
\xdefinecolor{ust-mustard}{rgb}{0.67,0.52,0.13}

%\newcommand\comment[1]{{\color{red}#1}}

\newcommand{\dy}{\partial}
\newcommand{\ddy}[2]{\frac{\dy#1}{\dy#2}}

\newcommand{\ex}{\mathrm{e}}
\newcommand{\zi}{{\rm i}}

\newcommand\Real{\mbox{Re}} % cf plain TeX's \Re and Reynolds number
\newcommand\Imag{\mbox{Im}} % cf plain TeX's \Im

\newcommand{\zbar}{{\overline{z}}}

\newcommand{\inner}[2]{\langle #1, #2\rangle}

\newcommand{\As}{{\mathsf{A}}}
\newcommand{\Bs}{{\mathsf{B}}}
\newcommand{\Ds}{{\mathsf{D}}}
\newcommand{\Is}{{\mathsf{I}}}
\newcommand{\Ps}{{\mathsf{P}}}
\newcommand{\Xs}{{\mathsf{X}}}
\newcommand{\Ys}{{\mathsf{Y}}}
\newcommand{\Ob}{{\boldsymbol{0}}}
\newcommand{\eb}{{\boldsymbol{e}}}
\newcommand{\ub}{{\boldsymbol{u}}}
\newcommand{\vb}{{\boldsymbol{v}}}
\newcommand{\wb}{{\boldsymbol{w}}}
\newcommand{\xb}{{\boldsymbol{x}}}
\newcommand{\yb}{{\boldsymbol{y}}}
\newcommand{\zb}{{\boldsymbol{z}}}

\newcommand\Def[1]{\textbf{#1}}

\newcommand{\qed}{\hfill$\blacksquare$}
\newcommand{\qedwhite}{\hfill \ensuremath{\Box}}

\newcommand{\highlight}[1]{\mathchoice%
  {\colorbox{black!10}{$\displaystyle#1$}}%
  {\colorbox{black!10}{$\textstyle#1$}}%
  {\colorbox{black!10}{$\scriptstyle#1$}}%
  {\colorbox{black!10}{$\scriptscriptstyle#1$}}}%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% some extra formatting (hacked from Patrick Farrell's notes)
%  https://courses.maths.ox.ac.uk/node/view_material/4915
%

% chapter format
\titleformat{\chapter}%
  {\huge\rmfamily\itshape\color{pastel-red}}% format applied to label+text
  {\llap{\colorbox{pastel-red}{\parbox{1.5cm}{\hfill\itshape\huge\color{white}\thechapter}}}}% label
  {1em}% horizontal separation between label and title body
  {}% before the title body
  []% after the title body

% section format
\titleformat{\section}%
  {\normalfont\Large\itshape\color{pastel-green}}% format applied to label+text
  {\llap{\colorbox{pastel-green}{\parbox{1.5cm}{\hfill\color{white}\thesection}}}}% label
  {1em}% horizontal separation between label and title body
  {}% before the title body
  []% after the title body

% subsection format
\titleformat{\subsection}%
  {\normalfont\large\itshape\color{pastel-blue}}% format applied to label+text
  {\llap{\colorbox{pastel-blue}{\parbox{1.5cm}{\hfill\color{white}\thesubsection}}}}% label
  {1em}% horizontal separation between label and title body
  {}% before the title body
  []% after the title body

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

% Front matter
%\frontmatter

% r.3 full title page
%\maketitle

% v.4 copyright page

\chapter*{}

\begin{fullwidth}

\par \begin{center}{\Huge Linera Algebra 2H}\end{center}

\vspace*{5mm}

\par \begin{center}{\Large typed up by B. S. H. Mithrandir}\end{center}

\vspace*{5mm}

\begin{itemize}
  \item \textit{Last compiled: \monthyear}
  \item Blended from notes of C. Keaton and J. R. Parker, Durham
  \item This was part of the Durham core second year modules. Involves
  introduction to vector spaces, linear maps, and matrices as linear maps
  \item I have personally changed the ordering quite a bit, since the ordering
  in my original I have from the original notes is obscenely jumpy and makes no
  logical sense to me...(if I were teaching this I would probably have matrices
  before linear maps)
\end{itemize}

\par

\par Licensed under the Apache License, Version 2.0 (the ``License''); you may
not use this file except in compliance with the License. You may obtain a copy
of the License at \url{http://www.apache.org/licenses/LICENSE-2.0}. Unless
required by applicable law or agreed to in writing, software distributed under
the License is distributed on an \smallcaps{``AS IS'' BASIS, WITHOUT WARRANTIES
OR CONDITIONS OF ANY KIND}, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
\end{fullwidth}


%===============================================================================

\chapter{Vector spaces}

Recall that a \textbf{field} $\mathbb{F}$ equipped with an addition $+$ and a
multiplication $\times$ operation satisfies the following axioms:
\begin{itemize}
  \item operations are associative
  \item operations are commutative
  \item there exists additive and multiplicative identity
  \item there exists additive and multiplicative inverse (except for the
  additive identity)
  \item multiplication is distributive over addition
\end{itemize}

The usual rationals $\mathbb{Q}$, reals $\mathbb{R}$ and complex numbers
$\mathbb{C}$ are common fields.

A set $V$ is a \textbf{vector space} over a field $\mathbb{F}$ if, for $\vb, \ub \in V$ and $\lambda, \mu \in \mathbb{F}$,
\begin{itemize}
  \item $V$ form an abelian group under addition $+$
  \item for all $\lambda\vb \in V$
  \item $(\lambda + \mu)\vb = \lambda \vb + \mu \vb$
  \item $\lambda(\ub + \vb) = \lambda \ub + \lambda \vb$
  \item $\lambda (\mu \vb) = (\lambda \mu) \vb$
  \item there exists an identity element $1$ where $1\vb = \vb$.
\end{itemize}
Some examples are
\begin{itemize}
  \item $\mathbb{F} = \mathbb{R}$ and $V = \mathbb{C}$
  \item $\mathbb{F} = \mathbb{R}$ and $V = \mathbb{R}^n$ is the Eucliean $n$-space
  \item $\mathbb{F} = \mathbb{R}$ and $V = C[0, 1]$ the set of continuous
  functions defined over the interval $[0, 1]$
  \item $\mathbb{F} = \mathbb{R}$ and $V$ the solutions to the ODE $y'' - y' + y
  = 0$
  \item $\mathbb{F} = \mathbb{R}$ and $V = \mathbb{R}[t]$, the set of
  polynomials in $t$ with coefficients in $\mathbb{R}$
\end{itemize}

\begin{lemma}
  If $V$ is a vector space over $\mathbb{F}$, then for all $\vb \in V$, we have
  $0\vb = \boldsymbol{0}$.
\end{lemma}

\begin{proof}
  We have $0\vb = (0 + 0)\vb = 0\vb + 0\vb$, so subtracting accordingly leads to
  the above result. \qed
\end{proof}

\begin{lemma}
  As above, but for all $\lambda \in \mathbb{F}$, we have $\lambda
  \boldsymbol{0} = \boldsymbol{0}$. \qedwhite
\end{lemma}

\begin{lemma}
  For all $\vb\in V$, $(-1)\vb = -\vb$, the additive inverse of $\vb$.
\end{lemma}

\begin{proof}
  $\boldsymbol{0} = 0\vb = (1 + (-1))\vb = \vb + (-1)\vb$, so that $(-1)\vb =
  -\vb$. \qed
\end{proof}

Let $U \subseteq V$, then $U$ is a \textbf{subspace} of $V$ over $\mathbb{F}$ if $U$ forms a vector space under the same addition and scalar multiplication as $V$.

Instead of checking that a given subspace $U$ really satisfies all the vector
space axioms, the follow theorem reduces the amount of work required.

\begin{theorem}
  $U$ is a subspace of $V$ if and only if $U \neq \emptyset$, and for all $\ub,
  \vb \in U$ and $\lambda \in \mathbb{F}$, we have $\ub + \vb \in U$ and
  $\lambda \vb \in U$.
\end{theorem}

\begin{proof}
  These conditions are obviously necessary for $U$ to be a vector space.
  Conversely, if $\ub, \vb \in U$, then $\ub$, $-\vb = (-1)\vb \in U$ by above
  lemma.s Since $\ub - \vb \in U$, $U \neq \emptyset$, and $U$ is a group under
  addition. The condition $\lambda \vb in U$ ensures that $U$ is closed. Since
  the relevant axioms hold in $V$, they also hold in $U$. \qed
\end{proof}

\begin{corollary}
  Let $U$ and $V$ be subspaces of $W$, then $U \cap V$ is a subspace of $W$.
\end{corollary}

%-------------------------------------------------------------------------------

\section{Bases}

We say the vectors $\{\vb_i\}$ are \textbf{linearly independent} if
\begin{equation}
  \lambda_1 \vb_1 + \ldots + \lambda_n \vb_n = 0
\end{equation}
only has trivial solutions $\lambda_i = 0$ for all $i$, if $\vb_i \neq \vb_j$
for $i\neq j$.

\begin{example}
  \begin{enumerate}
    \item Show
    \begin{equation*}
      S = \left\{\begin{pmatrix}1 \\ 0\\ 1\end{pmatrix}, \begin{pmatrix}0 \\ 1\\ 1\end{pmatrix}, \begin{pmatrix}2 \\ 0\\ 1\end{pmatrix}\right\}
    \end{equation*}
    are linearly independent.
    
    We have $a+2c = 0$, $b=0$, and $a+b+c=0$. This ends up reducing down
    accordingly to $a=b=c=0$, so $S$ is linearly independent.
    
    \item Shown
    \begin{equation*}
      S = \left\{\begin{pmatrix}1 \\ 0\\ 1\end{pmatrix}, \begin{pmatrix}0 \\ 1\\ 1\end{pmatrix}, \begin{pmatrix}1 \\ 1\\ 2\end{pmatrix}\right\}
    \end{equation*}
    is linearly dependent.
    
    Note that the third vector is the sum of the first and second vector.
    
    \item Show that $S=\{\sin nx\ |\ n\in\mathbb{N}\}$ is linearly
    independent.
    
    Note that we have the following orthogonality rule:\marginnote{Essentially
    from compound angle formula.}
    \begin{equation*}
      \int_{-\pi}^{\pi} \sin px \sin qx\, \mathrm{d}x = \begin{cases}\pi, & p = q,\\0, & p\neq q.\end{cases}
    \end{equation*}
    Suppose there exists some $\{b_i\}$ where $\sum_{k=1}^N b_k\sin kx = 0$ then
    we have, for some $p\in \{1, \ldots N\}$,
    \begin{equation*}
      \sum_{k=1}^N b_k \int_{-\pi}^\pi \sin px \sin kx\, \mathrm{d}x = 0,
    \end{equation*}
    so by the orthogonality condition, then $b_p \pi = 0$, and hence $b_p=0$ for
    arbitrary $p$, and so we have linear independence.\marginnote{This is one
    step towards saying sines and cosines forms the Fourier basis, which is
    important for the theorem of \emph{Fourier transforms}.}
  \end{enumerate}
\end{example}

The \textbf{span} of $\{\vb_i\}$ is defines as the set containing all linear
combinations of the \textbf{spanning set} $\{\vb_i\}$, i.e.
\begin{equation}
  \mbox{Span}(\{\vb_i\}) = \left\{ \sum_{i=1}^n \lambda_i \vb_i \ |\ \lambda_i \in \mathbb{F} \right\}.
\end{equation}

\begin{example}
  \begin{enumerate}
    \item $S = \{\eb_1, \eb_2, (2, 3, 0)\}$ spans only the $xy$ plane, because
    the third vector is not linearly independent of the other two.
    
    \item Does $S=\{\sin nx\ |\ n\in\mathbb{N}\}$ span $C^\infty(\mathbb{R})$?
    
    Suppose a function $f\in \mbox{Span}(S)$, then if $S$ spans
    $C^\infty(\mathbb{R})$, we should have, for all $x\in \mathbb{R}$,
    \begin{equation*}
      f(x) = \sum_{k=1}^n b_k \sin kx.
    \end{equation*}
    Here $f(0)=0$, but note that $\cos(x)\in C^\infty(\mathbb{R})$, but $\cos 0
    =1$, so $g\not\in \mbox{Span}(S)$, so the answer is no by a counter-example.
  \end{enumerate}
\end{example}

The set $\{\vb_i\}$ is a \textbf{basis} of some vector space $U$ if the spanning
set is linearly independent, and spans $U$. For
\begin{equation}
  \vb = \sum_{i=1}^n v_i \vb_i,
\end{equation}
we say $\vb$ has \textbf{co-ordinates} $(v_1, \ldots, v_n)$ with respect to the
basis $\{\vb_i\}$.

\begin{lemma}
  Let $S = \{\vb_1, \ldots \vb_n\}$ be a basis for a non-trivial $V$ over
  $\mathbb{F}$. Then every non-zero element of $V$ can be expressed
  \emph{uniquely} by a linear combination of elements of $S$.
\end{lemma}

\begin{proof}
  Let $\vb \neq \boldsymbol{0} \in V$. Since $S$ is a spanning set, we have $\vb
  = \sum_{k=1}^n \lambda_k \vb_k$ for some constants $\lambda_i \in \mathbb{F}$.
  Suppose $\vb$ has a different representation, i.e. $\vb = \sum_{k=1}^n \mu_k
  \vb_k$ for $\mu_k \in \mathbb{F}$, then we must have
  \begin{equation*}
    \sum_{k=1}^n (\lambda_k - \mu_k)\vb_k = 0,
  \end{equation*}
  but since $S$ is linearly independent, we must have $\lambda_k = \mu_k$, and
  so we have a unique representation. \qed
\end{proof}

\begin{theorem}
  Let $A = \{\ub_1, \ldots \ub_m\}$ and $B = \{\vb_1, \ldots \vb_n\}$ be bases of
  $V$. Then $n=m$.
\end{theorem}

\begin{proof}
  The $n$ or $m=0$ case is trivial. Without loss of generality, assume $0 < m
  \leq n$. Since $B$ is a basis for $V$, the first element of $A$ could be
  represented as $\ub_1 = \sum_{k=1}^n \lambda_k \vb_k$. Since $\ub_1 \neq
  \boldsymbol{0}$, the $\lambda$ are not all zero. Without loss of generality,
  suppose $\lambda_1 \neq 0$, then
  \begin{equation*}
    \vb_1 = \frac{1}{\lambda_1}(\ub_1 - \lambda_2\vb_2 \ldots - \lambda_n \vb_n),
  \end{equation*}
  so $\vb_1 \in \mbox{Span}(\{\ub_1, \vb_2, \ldots \vb_n\})$. The spanning set
  here is linearly independent, since if it wasn't, then there exists some
  $\mu_i$ where
  \begin{equation*}
    \mu_1 \ub_1 + \mu_2 \vb_2 + \ldots \mu_n \vb_n = 0,
  \end{equation*}
  and following the argument above, we can assume $\mu_1 \neq 0$ without loss of
  generality since $B$ is linearly independent, so $\ub_1 \in
  \mbox{Span}(\{\vb_2, \ldots \vb_n \}$, but by lemma, the representation of
  $\ub_1$ is unique, and since we assumed $\lambda_1 \neq 0$, we get a
  contradiction. 
  
  Note also that
  \begin{equation*}
    \mbox{Span}(\{\ub_1, \vb_1, \ldots \vb_n\}) = \mbox{Span}(\{\ub_1, \vb_2, \ldots \vb_n\}) = V,
  \end{equation*}
  so $\{\ub_1, \vb_2, \ldots \vb_n\}$ serves as a basis for $V$. We can then
  continue the argument above for $\vb_2, \ldots \vb_n$, and if $m<n$, we reach
  the conclusion that
  \begin{equation*}
    C = \{\ub_1, \ldots \ub_m, \vb_{m+1} \ldots \vb_n\}
  \end{equation*}
  is a basis for $V$. However, since $A$ is also a basis of $V$, is $\vb_n \in
  \mbox{Span}(A)$, then $C$ would not be linearly independent, and we have a
  contradiction. So $m<n$ is false, and $m=n$ is the only possibility. \qed
\end{proof}

If a basis for a vector space $V$ has $n$ elements, then the vector space
\textbf{dimension} $\mbox{dim}(V) = n$.

\begin{corollary}\label{cor:extend}
  Let $V$ be a $n$-dimensional vector space over $\mathbb{F}$, and $A = \{\eb_1,
  \ldots \eb_r\}$ be a subset of $V$ which is linearly independent. Then $A$ can
  be extended to a basis $\{\eb_1, \ldots \eb_n\}$ of $V$, where $r \leq n$.
\end{corollary}

\begin{proof}
  Let $B = \{\vb_1, \ldots \vb_n \}$ be a basis of $V$. If $\mbox{Span}(A) = V$
  then we are done, otherwise there exists $\eb_{r+1} \in B$ such that
  $\eb_{r+1} \not\in \mbox{Span}(A)$.
  
  Suppose $\lambda_1\eb_1 + \ldots + \lambda_{r+1}\eb_{r+1} = \boldsymbol{0}$
  for some $\lambda_i \in \mathbb{F}$. If $\lambda_{r+1} \neq 0$ then $\eb_{r+1}
  \in \mbox{Span}(A)$, which is a contradiction, so $\lambda_{r+1} = 0$, hence
  $A \cup \{\eb_{r+1}\}$ is a linearly independent set. We continue as necessary
  until we get $\{\eb_1, \ldots \eb_n\}$. \qed
\end{proof}

\begin{corollary}
  Let $V$ be a vector space over $\mathbb{F}$ with finite dimensions, and $U$ a
  subspace of $V$. Then $\mbox{dim}(U) \leq \mbox{dim}(V)$, and if
  $\mbox{dim}(U) = \mbox{dim}(V)$, then $U = V$.
\end{corollary}

\begin{proof}
  Assume $U$ is non-trivial, so there exists non-zero $\eb_1 \in U$. If
  $\mbox{Span}(\eb_1) = U$ then we are done, but otherwise there exists $\eb_2
  \in U - \mbox{Span}(\eb_1)$, and by previous theorem, $\{\eb_1, \eb_2\}$ is a
  linearly independent set. We can continue as necessary for $r \leq n$, and by
  previous corollary we can continue until a basis of $U$ is obtained. If
  $\mbox{dim}(U) = \mbox{dim}(V)$ then $r=n$, so $U=V$. qed
\end{proof}

\begin{corollary}
  Let $A = \{\eb_1, \ldots \eb_r\}$ be a subset of vector space $V$ over
  $\mathbb{F}$. There exists a subset which is a basis of $\mbox{Span}(A)$, and
  hence $\mbox{dim}(\mbox{Span}(A)) \leq r$.
\end{corollary}

\begin{proof}
  If $A$ is linearly independent it is a basis, so $\mbox{dim}(\mbox{Span}(A)) =
  r$. Otherwise, there exists non-trivial $\lambda_i \in \mathbb{F}$ such that
  $\sum_{i=1}^n \lambda_i \eb_i = \boldsymbol{0}$. Assume $\lambda_r \neq 0$,
  then $\eb_r \in \mbox{Span}(A - \{\eb_r\})$. If $\mbox{Span}(A - \{\eb_r\})$
  is linearly independent then it is a basis and $\mbox{dim}(\mbox{Span}(A -
  \{\eb_r\}) = r-1 < r$ as. Otherwise continue as necessary. \qed
\end{proof}

\begin{example}
  \begin{enumerate}
    \item Find a subset of
    \begin{equation*}
      S = \left\{\begin{pmatrix}1 \\ -1\\ 2\end{pmatrix}, \begin{pmatrix}2 \\ 1\\ 1\end{pmatrix}, \begin{pmatrix}0 \\ 1\\ -1\end{pmatrix}\right\}
    \end{equation*}
    which is a basis of $\mbox{Span}(S)$.
    
    Notice that the second vector is a linear combination of the other two, so
    we could discard that to get $S'$ which serves as a basis for
    $\mbox{Span}(S)$.
    
    \item Extend $S'$ to make it a basis for $\mathbb{R}^3$.
    
    Note that for $\eb_i$, only $\eb_1$ is not in $\mbox{Span}(S')$, so we can
    augment $S'$ with $\eb_1$.
  \end{enumerate}
\end{example}

%-------------------------------------------------------------------------------

\section{Sums and direct sums}

Let $U$ and $V$ be vector spaces over $\mathbb{F}$. Then \textbf{sum} of $U$ and
$V$ is
\begin{equation}
  \highlight{U + V = \{\ub, \vb\ |\ \ub \in U, \vb \in V \}}.
\end{equation}

\begin{lemma}
  If $U$ and $V$ are subspaces of $W$, then $U+V$ is also a subspace of $W$.
\end{lemma}

\begin{proof}
  $U+V$ is non-empty since $\boldsymbol{0} + \boldsymbol{0} = \boldsymbol{0} \in
  U+V$. Let $\ub$, $\ub' \in U$ and $\vb$, $\vb' \in V$, then
  \begin{equation*}
    \lambda(\ub + \vb) = \lambda\ub + \lambda\vb \in U+V
  \end{equation*}
  for all $\lambda \in \mathbb{F}$. Also,
  \begin{equation*}
    (\ub+\vb) + (\ub'+\vb') = (\ub + \ub') + (\vb + \vb') \in U+V,
  \end{equation*}
  so $U+V$ is also closed, and thus a subspace.
\end{proof}

The following theorem is useful for determining dimensions of spaces.

\begin{theorem}\label{thm:subspace_dim}
  Let $U$ and $V$ be finite subspaces of $W$ over $\mathbb{F}$. Then
  \begin{equation}
    \highlight{\mbox{dim}(U) + \mbox{dim}(V) = \mbox{dim}(U \cap V) + \mbox{dim}(U+V)}.
  \end{equation}
\end{theorem}

\begin{proof}
  By Corollary~\ref{cor:extend}, there exists a basis $B=\{\eb_1, \ldots
  \eb_r\}$ which is a basis of $U \cap V$ that may be extended to $\{\eb_1,
  \ldots \eb_r, \boldsymbol{f}_1, \ldots \boldsymbol{f}_s\}$ of $U$, and
  $\{\eb_1, \ldots \eb_r, \boldsymbol{g}_1, \ldots \boldsymbol{g}_t\}$ of $V$.
  
  Clearly $\{\eb_1, \ldots \eb_r, \boldsymbol{f}_1, \ldots \boldsymbol{f}_s,
  \boldsymbol{g}_1, \ldots \boldsymbol{g}_t\}$ spans $U+V$. Suppose there exists
  non-trivial scalars $\lambda_i$, $\mu_i$ and $\nu_i$ such that
  \begin{equation*}
    \sum_{i=1}^r \lambda_i\eb_i + \sum_{i=1}^s \mu_i\boldsymbol{f}_i + \sum_{i=1}^t \nu_i\boldsymbol{g}_i = \boldsymbol{0}.
  \end{equation*}
  Then not all of $\nu_i$ is zero, since $\{\eb_1,
  \ldots \eb_r, \boldsymbol{f}_1, \ldots \boldsymbol{f}_s\}$ is a basis of of
  $U$ and hence linearly independent (and similarly for the other case). Then
  $\{\boldsymbol{g}_1, \ldots \boldsymbol{g}_t\}$ is linearly independent and
  hence in $U$, so much be in $U \cap V$. Then
  \begin{equation*}
    \sum_{i=1}^t \nu_i\boldsymbol{g}_i = \sum_{i=1}^r \lambda_i\eb_i,
  \end{equation*}
  but this cannot be since $\{\eb_1, \ldots \eb_r, \boldsymbol{g}_1, \ldots
  \boldsymbol{g}_t\}$ is a basis of $V$. So then $\{\eb_1, \ldots \eb_r,
  \boldsymbol{f}_1, \ldots \boldsymbol{f}_s, \boldsymbol{g}_1, \ldots
  \boldsymbol{g}_t\}$ is a basis of $U+V$. Counting dimensions accordingly leads
  to the result. \qed
\end{proof}

\begin{example}
  Find a basis of $U \cap V$, and hence determine $\mbox{dim}(U)$,
  $\mbox{dim}(V)$, $\mbox{dim}(U \cap V)$, and $\mbox{dim}(U+V)$, where
  \begin{equation*}
    U = \mbox{Span}\left(\left\{\begin{pmatrix}1 \\ 0\\ 1\end{pmatrix}, \begin{pmatrix}2 \\ 2\\ 0\end{pmatrix}\right\}\right), \quad V = \mbox{Span}\left(\left\{\begin{pmatrix}1 \\ -1\\ -1\end{pmatrix}, \begin{pmatrix}2 \\ 1\\ -2\end{pmatrix}\right\}\right).
  \end{equation*}
    
  We see the spanning sets of $U$ and $V$ are linearly independent, hence
  $\mbox{dim}(U) = \mbox{dim}(V) =2$. We note that $\wb \in U \cap V$ can be
  expressed as
  \begin{equation*}
    \wb = a \begin{pmatrix}1 \\ 0\\ 1\end{pmatrix} + b \begin{pmatrix}2 \\ 2\\ 0\end{pmatrix} = c \begin{pmatrix}1 \\ -1\\ -1\end{pmatrix} + d \begin{pmatrix}2 \\ 1\\ -2\end{pmatrix},
  \end{equation*}
  and the resulting system implies that $a=c$ and $b=d=-c$. Taking $c=1$ leads
  to
  \begin{equation*}
    U \cap V = \mbox{span}\left(\begin{pmatrix}-1 \\ -2\\ 1\end{pmatrix}\right),
  \end{equation*}
  and hence $\mbox{dim}(U \cap V) = 1$. By Theorem~\ref{thm:subspace_dim}, we
  should have $\mbox{dim}(U + V) = 2+2-1 = 3$, which can also be checked
  directly.
\end{example}

The \textbf{direct sum} of $U$ and $V$ is denoted $\highlight{U \bigoplus V}$, which is $U + V$ but where $U \cap V = \{\boldsymbol{0}\}$.

\begin{theorem}
  Let $U$ and $V$ be subspaces of a vector space. Then the $U + V$ is a direct
  sum if and only if there exists unique $\ub \in U$, $\vb \in V$ where for all
  $\wb = \ub + \vb \in W$.
\end{theorem}

\begin{proof}
  If $W = U \bigoplus V$ then $\wb = \ub + \vb$ by definition, so we need to
  shown uniqueness. Suppose $\wb = \ub' + \vb'$, then
  \begin{equation*}
    \ub' + \vb' = \ub + \vb \quad \Leftrightarrow \quad \vb' - \vb = \ub - \ub'.
  \end{equation*}
  We note that $\vb' - \vb, \ub - \ub' \in U \cap V$, so by definition they are
  just the zero vector, and we have uniqueness.
  
  Let $\wb \in U \cap V$ and assume we have uniqueness. So we must have $\wb =
  \wb + \boldsymbol{0}_V = \boldsymbol{0}_U + \wb$ for the respective zero
  vectors, but the above is only true if $\wb = \boldsymbol{0}$, and hence $U
  \cap V = \{\boldsymbol{0}\}$ and we have a direct sum. \qed
\end{proof}

\begin{example}
  If $U = \mbox{Span}(\{\eb_1\})$ and $V = \mbox{Span}(\{\eb_3\})$ then $W = U
  \bigoplus V = \mbox{Span}(\{\eb_1, \eb_3\})$.
  
  On the other hand, $U = \mbox{Span}(\{\eb_1 + \eb_2\})$ and $V =
  \mbox{Span}(\{\eb_2 + \eb_3\})$, the sum is not direct, and we have $W = U + V
  = \mbox{Span}(\{\eb_1, \eb_2, \eb_3\})$.
\end{example}

Let $U$ and $V$ be vector spaces over $\mathbb{F}$. Then $\highlight{U \times
V}$ is the \textbf{external direct sum}, which is a vector space with the
definition that the ordered pairs satisfy
\begin{equation*}
  (u, v) + (u', v') = (u + u', v + v'), \quad \lambda(u, v) = (\lambda u, \lambda v).
\end{equation*}

%-------------------------------------------------------------------------------

\section{Inner products}

%-------------------------------------------------------------------------------

\section{Gram--Schmidt orthonomalisation}

%-------------------------------------------------------------------------------

\section{Orthogonal projection}

%===============================================================================

\chapter{Linear maps}

A \textbf{linear map} $\theta : U \to V$ for vector spaces over $\mathbb{F}$ is
a function that satisfies
\begin{enumerate}
  \item $\theta(\ub + \vb) = \theta(\ub) + \theta(\vb)$ for all $\ub, \vb \in U$,
  \item $\theta(\lambda \ub) = \lambda \theta(\ub)$ for all $\ub \in U$,
  $\lambda \in \mathbb{F}$.
\end{enumerate}

Note that $\theta$ is a \emph{homomorphism}, respecting scalar multiplication.

\begin{lemma}
  \item The composition of linear maps is linear.
\end{lemma}

\begin{proof}
  We have
  \begin{align*}
    \phi \circ \theta(\ub + \vb) = \phi(\theta(\ub + \vb)) &= \phi(\theta(\ub) + \theta(\vb)) \\
      & = \phi(\theta(\ub)) + \phi(\theta(\vb)) \\
      & = \phi \circ \theta(\ub) + \phi \circ \theta(\vb),
  \end{align*}
  and
  \begin{align*}
    \phi \circ \theta(\lambda\ub) = \phi(\theta(\lambda\ub)) = \phi(\lambda\theta(\ub)) = \lambda\phi(\theta(\ub)) = \lambda \phi \circ \theta(\ub).
  \end{align*}
  \qed
\end{proof}

Suppose $\theta : U \to V$ is a linear map, then we define\marginnote{Kernel is
the set things that get sent to zero, while the image is the set of things that
gets mapped by $\theta$.}
\begin{itemize}
  \item \textbf{kernel} of $\theta$ as
  \begin{equation}
    \highlight{\mbox{Ker}(\theta) = \{\ub \in U\ |\ \theta(\ub) = \boldsymbol{0}\}},
  \end{equation}
  \item \textbf{image} of $\theta$ as
  \begin{equation}
    \highlight{\mbox{Im}(\theta) = \{\theta(\ub) \ |\ \theta(\ub) \in U\}}.
  \end{equation}
\end{itemize}

\begin{lemma}
  The kernel and image of $\theta$ are subspaces of $U$ and $V$ respectively.
\end{lemma}

\begin{proof}
  Suppose $\ub, \vb \in \mbox{Ker}(\theta)$, then note that
  \begin{equation*}
    \boldsymbol{0} = \theta(\ub) - \theta(\vb) = \theta(\ub - \vb)
  \end{equation*}
  since $\theta$ is a homomorphism, so that $\ub - \vb \in \mbox{Ker}(\theta)$,
  while
  \begin{equation*}
    \boldsymbol{0} = \lambda \boldsymbol{0} = \lambda \theta(\ub) = \theta(\lambda\ub)
  \end{equation*}
  by property of a linear map, so $\lambda\ub \in \mbox{Ker}(\theta)$. So we
  have closure and therefore the kernel is a subspace of $U$.
  
  Suppose $\boldsymbol{a}, \boldsymbol{b} \in \mbox{Im}(\theta)$, then there
  exists $\boldsymbol{p}, \boldsymbol{q} \in U$ where $\theta(\boldsymbol{p}) =
  \boldsymbol{a}$ and $\theta(\boldsymbol{q}) = \boldsymbol{b}$. Then similar
  manipulations shows the image is closed, and is therefore a subspace of $V$. \qed
\end{proof}

%-------------------------------------------------------------------------------

\section{Spaces of linear maps}

%-------------------------------------------------------------------------------

\section{Dual space}

%===============================================================================

\section{Matrices}

Mostly going to assume we are working with matrices defined over the reals,
unless stated otherwise.

%-------------------------------------------------------------------------------

\section{Elementary matrices}

%-------------------------------------------------------------------------------

\section{Rank}

Let $\As$ be a $m\times n$ matrix. Then the \textbf{row rank} of $\As$ is the
dimension of the subspaces spanned by the row vectors of $\As$. The
\textbf{column rank} is defined similarly.

\begin{theorem}
  For every matrix, the row rank is equal to the column rank (so we only need to
  talk about \emph{rank}).
\end{theorem}

\begin{proof}
  Think of $\As$ as a linear map $\theta: \mathbb{R}^n \to \mathbb{R}^m$, so
  there exists non-singular $\Xs$ and $\Ys$ where
  \begin{equation*}
    \Bs = \Xs \As \Ys = \begin{pmatrix}\Is_r & 0 \\ 0 & 0 \end{pmatrix},
  \end{equation*}
  where $r$ is the row rank of $\theta$. The transpose of $\Bs$ has the same
  rank, although that is now the column rank of $\Bs^T$. \qed
\end{proof}

We define the \textbf{nullity} of $\As$ to be the dimension of the kernel, i.e.,
the dimension of the solution set $\As\xb = \Ob$.

\begin{lemma}
  The following are equivalent if $\As$ is a square $n\times n$ matrix:
  \begin{enumerate}
    \item $\As$ is invertible,
    \item $\mbox{rank}(\As) = n$,
    \item $\mbox{null}(\As) = 0$.
  \end{enumerate}
\end{lemma}

\begin{proof}
  If $\As$ is invertible then the corresponding linear map $\theta$ is an
  isomorphism, and we have the result by a previous corollary.
  {\color{red}reference back} \qed
\end{proof}

%-------------------------------------------------------------------------------

\section{Trace and determinant}

%-------------------------------------------------------------------------------

\section{Adjugate}

%-------------------------------------------------------------------------------

\section{Eigenvalues and eigenvectors}

Let $V$ be a vector space, and $\theta:V \to V$ be a linear map. Then $\vb\in
V-\{\Ob\}$ is an \textbf{eigenvector} of $\theta$ if
\begin{equation}
  \highlight{\theta(\vb) = \lambda \vb},
\end{equation}
where $\lambda$ is the corresponding \textbf{eigenvalue}. The subset
\begin{equation}
  V_{\lambda} = \{\vb\in V \ |\ \theta(\vb) = \lambda \vb\}
\end{equation}
is a vector space called the \textbf{eigenspace} corresponding to the eigenvalue
$\lambda$.

Clearly if $\As$ is the corresponding matrix to $\theta$ then the eigenvalues
and eigenvectors would correspond accordingly.

Two $n\times n$ matrices $\As$ and $\Bs$ are called \textbf{similar} if there
exists some non-singular $\Ys$ where $\Bs = \Ys^{-1} \As \Ys$.

\begin{theorem}
  If $\As$ corresponds to the linear map $\theta:V\to V$, then the roots of the
  \textbf{characteristic polynomial} $\highlight{|\lambda\Is - \As| = 0}$ are
  the eigenvalues of $\theta$. This is independent of the basis of $V$.
\end{theorem}

\begin{proof}
  Let $\vb$ be an eigenvector, so that $\vb=\sum_i v_i\eb_i$. If $\lambda$ is
  the corresponding eigenvalue, and $\textsf{A}$ is the representation of
  $\theta$ in the basis $\{\eb_i\}$, then
  \begin{equation*}
    \As\vb = \lambda\vb \quad \Leftrightarrow \quad (\lambda\Is - \As)\vb = \Ob.
  \end{equation*}
  Since $\vb$ is non-trivial by definition of it being an eigenvector, then
  $\lambda\Is - \As$ is singular, so $|\lambda\Is - \As| = 0$.
  
  For basis independence, we know that
  \begin{align*}
    |\lambda\Is - \Ys^{-1}\As\Ys| &= |\lambda\Ys^{-1}\Is\Ys - \Ys^{-1}\As\Ys| \\
      & = |\Ys^{-1}(\lambda\Is- \As)\Ys|\\
      & = |\Ys^{-1}| \cdot |\lambda\Is- \As| \cdot |\Ys|,
  \end{align*}
  but by the property that $|\Ys^{-1}| \cdot |\Ys| = 1$, we have $|\lambda\Is -
  \Ys^{-1}\As\Ys| = |\lambda\Is- \As| = 0$. \qed
\end{proof}

\begin{example}
  For
  \begin{equation*}
    \As = \begin{pmatrix}2 & 2 \\ 0 & 4\end{pmatrix}, \quad \lambda\Is - \As = \begin{pmatrix}\lambda - 2 & -2 \\ 0 & \lambda - 4\end{pmatrix}
  \end{equation*}
  and the eigenvalues satisfy\marginnote{Could have done this with the trace and
  determinant also for a $2\times2$ matrix.} the quadratic
  $(\lambda-2)(\lambda-4) = 0$, so $\lambda=2, 4$. For $\lambda=2$, the
  eigenvector satisfies
  \begin{equation*}
    \begin{pmatrix}0 & -2 \\ 0 & -2\end{pmatrix}\begin{pmatrix}v_1 \\ v_2 \end{pmatrix} = \Ob,
  \end{equation*}
  which implies $v_2 = 0$ but $v_1$ is undetermined; we could choose for
  simplicity for the eigenvector to be
  \begin{equation*}
   \vb_{\lambda=2} = \begin{pmatrix}1 \\ 0 \end{pmatrix}.
  \end{equation*}
  For $\lambda = 4$, we result in $v_1 = v_2$, so we could choose
  \begin{equation*}
   \vb_{\lambda=4} = \begin{pmatrix}1 \\ 1 \end{pmatrix}.
  \end{equation*}
\end{example}

Notice for the above that if we take $\Ys$ to be the matrix whose column vectors
are the eigenvectors, i.e.
\begin{equation*}
  \Ys = \begin{pmatrix}1 & 0 \\ 1 & 1\end{pmatrix} \quad \Rightarrow \quad \Ys^{-1} = \begin{pmatrix}1 & -1 \\ 0 & 1\end{pmatrix},
\end{equation*}
then notice that 
\begin{equation*}
  \Ys^{-1} \As \Ys = \begin{pmatrix}1 & 0 \\ 1 & 1\end{pmatrix}\begin{pmatrix}2 & 2 \\ 0 & 4\end{pmatrix}\begin{pmatrix}1 & -1 \\ 0 & 1\end{pmatrix} = \begin{pmatrix}2 & 0 \\ 0 & 4\end{pmatrix},
\end{equation*}
which is just the \textbf{diagonal} matrix with the eigenvalues on the main
diagonal. This is called \textbf{diagonalisation}.

\begin{theorem}
  Let $\As$ be a $n\times n$ matrix. The matrix $\As$ is \textbf{diagonalisable}
  if there exists a non-singular $\Ys$ such that $\As$ is similar to a diagonal
  matrix $\Ds$, i.e. $\highlight{\Ds = \Ys^{-1} \As \Ys}$ or $\highlight{\As =
  \Ys \Ds\Ys^{-1}}$. This is equivalent to the condition that the eigenvectors
  of $\As$ has full span.
\end{theorem}

Notice we do not require $\As$ to be non-singular, nor that the eigenvalues have
to be distinct. For example,
\begin{equation*}
  \begin{pmatrix}1 & 0 \\ 0 & 0\end{pmatrix}, \quad \begin{pmatrix}1 & 0 \\ 0 & 1\end{pmatrix}
\end{equation*}
are respectively singular and have repeated eigenvalues, but are both trivially
diagonal (so diagonalisable by $\Is$). On the other hand,
\begin{equation*}
  \begin{pmatrix}0 & 1 \\ 1 & 0\end{pmatrix}
\end{equation*}
has eigenvalues $\lambda = \pm \zi$, so it is definitely not diagonalisable
\emph{over the reals}.

\begin{example}
  For
  \begin{equation*}
    \As = \begin{pmatrix}3 & 0 \\ 1 & 3\end{pmatrix},
  \end{equation*}
  we have $\lambda=3$ of multiplicity 2, but additionally only one eigenvector
  $(1,0)^T$, so the span of the eigenspace is only dimension 1, and matrix is
  not diagonalisable.
\end{example}

\begin{example}
  For
  \begin{equation*}
    \As = \begin{pmatrix}0 & -2 & 2 \\ 0 & 2 & 0 \\ -1 & -1 & 3\end{pmatrix},
  \end{equation*}
  can show that the eigenvalues are $\lambda=1$ and $\lambda=2$ twice. For
  eigenvector corresponding to $\lambda=1$ can be chosen to be
  \begin{equation*}
    \vb_1 = \begin{pmatrix}2 \\ 0 \\ 1\end{pmatrix}.
  \end{equation*}
  For $\lambda=2$, we reduce down to $v_1 + v_2 - v_3 = 0$, so we could take
  \begin{equation*}
    \vb_2 = \begin{pmatrix}1 \\ 0 \\ 1\end{pmatrix}, \quad \vb_3 = \begin{pmatrix}0 \\ 1 \\ 1\end{pmatrix},
  \end{equation*}
  which are linearly independent of each other, so the eigenspace has full span,
  and the transformation matrix could be
  \begin{equation*}
    \Ys = \begin{pmatrix}2 & 0 & 1 \\ 1 & 0 & 1 \\ 0 & 1 & 1\end{pmatrix}.
  \end{equation*}
  Notice we could have made a different choice for $\vb_{2,3}$, and
  that
  \begin{equation*}
    \Ys = \begin{pmatrix}2 & 0 & 1 \\ 1 & 0 & 1 \\ 2 & -1 & 1\end{pmatrix}
  \end{equation*}
  would have also worked.
\end{example}

There are at least two notable uses for this:
\begin{example}
  \begin{enumerate}
    \item If we have the system of ODEs
    \begin{equation*}
      \frac{\mathrm{d}}{\mathrm{d}t}\xb = \As\xb
    \end{equation*}
    where $\As$ is constant and diagonalisable with $\As = \Ys \Ds \Ys^{-1}$,
    then the solutions are
    \begin{equation*}
      \xb = \Ys\begin{pmatrix}\ex^{\lambda_1 t} \\ \vdots \\ \ex^{\lambda_n t}\end{pmatrix}
    \end{equation*}
    up to multiplicative constants (which are specified by the initial
    conditions). This is because
    \begin{equation*}
      \frac{\mathrm{d}}{\mathrm{d}t}\xb = \Ys \Ds \Ys^{-1}\xb \quad \Rightarrow \frac{\mathrm{d}}{\mathrm{d}t} \Ys^{-1}\xb = \Ds\Ys^{-1}\xb,
    \end{equation*}
    because $\Ys$ is constant in time so commutes with the derivative. Then
    calling $\yb = \Ys^{-1}\xb$, we just have a set of trivial constant ODEs to
    solve, then we invert to get $\xb = \Ys\yb$ which is known. 
    
    In other words, we choose a convenient basis where our problem is diagonal,
    solve the problem there (which is easy), and then transform it back into the
    original basis. This kind of problem occurs occasionally in quantum
    mechanics, where you might want to solve the \textbf{time-independent
    Schr\"odinger equation}
    \begin{equation*}
      \zi\hbar\frac{\mathrm{d}}{\mathrm{d}t}|\psi(t)\rangle = \mathsf{H}|\psi(t)\rangle
    \end{equation*}
    where $\mathsf{H}$ would be matrix representation of the relevant
    Hamiltonian of the system\marginnote{The Hamiltonian normally has additional
    properties that make the eigenvalues and eigenvectors have certain
    properties; more later when we talk about
    \emph{Hermitian}/\emph{self-adjoint} matrices.}, and $|\psi(t)\rangle$ would
    be the vector representation of the \textbf{state}. The eigenvalues
    correspond usually to an energy, and the eigenvectors the energy states.
    
    \item Suppose we have some system that is governed by
    \begin{equation*}
      \xb_{n+1} = \As\xb_n,
    \end{equation*}
    such as a \textbf{Markov chain} with discrete states, and so $\As$ is
    a matrix detailing the transition probabilities between states (with all
    positive entries that are strictly less than 1, since these are supposed to
    describe probabilities). If $\As$ is diagonalisable, then the
    numerics becomes `easy', in that to find $\xb_{n+1}$ from initial
    data $\xb_0$, we do
    \begin{align*}
      \xb_n &= \As^n\xb_0 \\ 
        &= (\Ys \Ds \Ys^{-1})^n \xb_0 \\
        &= (\Ys \Ds \Ys^{-1})(\Ys \Ds \Ys^{-1})\cdots(\Ys \Ds \Ys^{-1}) \xb_0 \\
        &= \Ys \Ds (\Ys^{-1}\Ys)\Ds\cdots(\Ys^{-1}\Ys)\Ds\Ys^{-1} \xb_0 \\
        &= \Ys \Ds^n \Ys^{-1} \xb_0,
    \end{align*}
    by associativity, and noting that $D^n$ is essentially trivial to
    compute.\marginnote{In the context of a Markov chain, the transition matrix
    have eigenvalues with magnitude less than or equal to 1 by the
    \emph{Perron--Frobenius theorem}, so we have no blow up. This is not
    necessarily true for population models such as the \emph{Leslie model},
    since the entries are positive but do not have to be strictly less than 1.}
  \end{enumerate}
\end{example}

%-------------------------------------------------------------------------------

\section{Cayley--Hamilton theorem}

%-------------------------------------------------------------------------------

\section{Orthogonal matrices}

Recall that two vectors are orthogonal to each other if $\ub\cdot\vb = \ub^T\vb
= 0$, the latter interpreted in terms of a matrix multiplication of a row vector
with a column vector. 

Analogously, a matrix is called \textbf{orthogonal} if the columns are
orthogonal to each other. However, a more useful form of orthogonality might be
the following:
\begin{theorem}
  A matrix $\As$ is orthogonal if and only if $\As^{-1} = \As^T$.\marginnote{So
  orthogonal matrices are great, because a transpose is much easier to compute
  than the inverse.}
\end{theorem}

\begin{proof}
  Let $\eb_i$ be the $i^{\textnormal{th}}$ column of $\Ps$, then $\As \As^T =
  (\eb_i^T \eb_j) = (\delta_{ij}) = \Is$, as required. \qed
\end{proof}

%-------------------------------------------------------------------------------

\section{Symmetric matrices}

A matrix is $\As$ is \textbf{symmetric} if $\highlight{\As = \As^T}$.

\begin{theorem}\label{thm:real_sym_eigen}
  Let $\As$ be a real symmetric matrix. Then the eigenvalues are all real, and
  the eigenvectors are mutual orthogonal to each other. \qedwhite
\end{theorem}

\begin{theorem}
  Let $\As$ be a real symmetric matrix. If it is diagonalisable, then the
  similarity matrix is orthogonal, i.e. $\As = \Ps \Ds \Ps^{-1} = \Ps \Ds
  \Ps^T$. \qedwhite
\end{theorem}

The theorems are special cases of those given in \S\ref{sec:hermitian}, so we
defer the proof for later.

\begin{example}
  Diagonalise
  \begin{equation*}
    \As = \begin{pmatrix}-1 & 1 & -1 \\ 1 & -1 & -1 \\ -1 & -1 & -1\end{pmatrix}.
  \end{equation*}
  
  Can show the eigenvalues are $1$, $-2$ and $-2$, and that
  \begin{equation*}
    \Ps = \begin{pmatrix}1 & 1 & 1 \\ 1 & 0 & -2 \\ -1 & 1 & 1\end{pmatrix}
  \end{equation*}
  is one possible choice, leading to $\Ds = \mbox{diag}(1, -2, -2)$.
\end{example}

%-------------------------------------------------------------------------------

\section{Rotations}

Denote the set of $n\times n$ orthogonal matrices over $\mathbb{R}$ by
$\mbox{O}(n)$, and $\mbox{SO}(n)$ the subset of $\mbox{O}(n)$ where the
corresponding matrices have determinant $+1$. These turn out to form a group
under matrix multiplication, and are known as the \textbf{orthogonal} and
\textbf{special orthogonal} groups of dimension $n$
respectively.\marginnote{These are actually \textbf{Lie groups}, which play an
important role in mathematical physics.}

\begin{theorem}
  Every element of $\mbox{SO}(3)$ represent a rotation about an arbitrary axis
  in $\mathbb{R}^3$. \qedwhite
\end{theorem}

\begin{corollary}
  The characteristic polynomial
  \begin{equation*}
    \Delta(\lambda) = \lambda^3 - (1+2\cos\theta)\lambda^2 + (1+2\cos\theta)\lambda - 1
  \end{equation*}
  tells us the angle of rotation $\theta$ (via $\cos\theta$, up to a sign) of a
  rotation about any axis.
\end{corollary}

\begin{example}
  Find the matrix which represents a rotation in $\mathbb{R}^3$ about
  $(1,1,0)^T$ through an angle of $\pi/2$.
  
  Note that we have
  \begin{align*}
    \mathsf{R}_x(\theta) = \begin{pmatrix}1 & 0 & 0 \\ 0 & \cos\theta & -\sin\theta \\ 0 & \sin\theta & \cos\theta\end{pmatrix}&, \quad
    \mathsf{R}_y(\theta) = \begin{pmatrix}\cos\theta & 0 & \sin\theta \\ 0 & 1 & 0 \\ -\sin\theta & 0 & \cos\theta\end{pmatrix}, \\
    \mathsf{R}_z(\theta) = &\begin{pmatrix}\cos\theta & -\sin\theta & 0 \\ \sin\theta & \cos\theta & 0 \\ 0 & 0 & 1\end{pmatrix}
  \end{align*}
  are the rotation matrices about the $x$, $y$ and $z$ axis respectively by an
  angle $\theta$. Then idea is that the axis is represented by a vector that we
  can normlalise, and we rotate that onto a standard axis (essentially a
  co-ordinate transformation), do the rotation by the desired angle (which we
  know the matrix of), and then undo rotation (or the co-ordinate
  transformation).
  
  We can take the $z$ axis for definiteness, and we want $\Ps:
  (1,1,0)^T/\sqrt{2} \mapsto (0,0,1)^T$. This is supposed to be a rotation, so
  $\Ps^{-1}$ exists and $\Ps^{-1}:(0,0,1)^T \mapsto (1,1,0)^T/\sqrt{2}$, or
  \begin{equation*}
    \Ps^{-1} = \begin{pmatrix}\cdot & \cdot & 1/\sqrt{2} \\ \cdot & \cdot & 1/\sqrt{2} \\ \cdot & \cdot & 0\end{pmatrix}.
  \end{equation*}
  The column vectors should be mutually orthogonal to each other, and we can
  choose them to be orthonormal, and we could consider the simple choice
  \begin{equation*}
    \Ps^{-1} \stackrel{!?}{=} \begin{pmatrix}0 & -1/\sqrt{2} & 1/\sqrt{2} \\ 0 & 1/\sqrt{2} & 1/\sqrt{2} \\ 1 & 0 & 0\end{pmatrix}.
  \end{equation*}
  Notice however that $|\Ps^{-1}|=-1$, so to make sure $\Ps\in\mbox{SO}(3)$, we
  can simply multiply the bottom row by $-1$. The resulting $\Ps$ is then
  \begin{equation*}
    \Ps = \begin{pmatrix}0 & 1/\sqrt{2} & 1/\sqrt{2} \\ 0 & -1/\sqrt{2} & 1/\sqrt{2} \\ 1 & 0 & 0\end{pmatrix},
  \end{equation*}
  and so
  \begin{equation*}
    \As = \Ps^{-1} \mathsf{R}_z(\pi/2) \Ps = \begin{pmatrix}1/2 & 1/2 & 1/\sqrt{2} \\ 1/2 & 1/2 & -1/\sqrt{2} \\ -1/\sqrt{2} & 1/\sqrt{2} & 0\end{pmatrix}
  \end{equation*}
  will do the job. Can brute force check that that $|\As|=1$, $\As^T = \As$, and
  the only solution to $\As\xb = \xb$ is $\xb = a(1, 1, 0)^T$.\marginnote{The
  rotation axis is kept fixed by the rotation about that axis.}
\end{example}

%-------------------------------------------------------------------------------

\section{Hermitian matrices}\label{sec:hermitian}

{\color{red}remember to introduce somewhere star being the Hermitian transpose}

A matrix over $\mathbb{C}$ is \textbf{Hermitian} if $\highlight{\As = \As^* =
\overline{\As}^T}$. Note that if $\As$ is actually real, then this reduces to
the definition of a symmetric matrix.

A matrix over $\mathbb{C}$ is \textbf{unitary} if $\highlight{\As^* =
\As^{-1}}$.\marginnote{Unitary matrixes are used a lot in quantum mechanics.}
Again, if $\As$ is actually real, then this reduces to the definition of an
orthogonal matrix.

\begin{theorem}
  Let $\As$ be a Hermitian matrix. Then the eigenvalues are all real, and the
  eigenvectors are mutual orthogonal to each other.\marginnote{Compare this with
  Theorem \ref{thm:real_sym_eigen}. In quantum mechanics the Hamiltonians are
  usually (but not always) chosen to be Hermitian, so the eigenvalues are all
  real and observable.}
\end{theorem}

\begin{proof}
  Let $\lambda$ be an eigenvalue of $\As$, so $\As\vb=\lambda\vb$ with
  $\vb\neq\Ob$. Now,
  \begin{equation*}
    \lambda\|\vb\|^2 = \lambda\vb^*\vb = \vb^* (\lambda\vb) = \vb^* \As\vb = \vb^* \As^*\vb = (\As\vb)^* \vb = \overline{\lambda}\vb^*\vb = \overline{\lambda}\|\vb\|^2,
  \end{equation*}
  so $\lambda$ is real for non-zero $\vb$.
  
  Let $\lambda$ and $\mu$ be distinct eigenvalues of $\As$, so there exists
  associated eigenvectors $\vb$ and $\wb$ respectively. Then doing similar
  manipulations to above,
  \begin{equation*}
    \lambda\wb^*\vb = \wb^* \As\vb = (\As\wb)^* \vb = \mu\wb^*\vb.
  \end{equation*}
  Since $\lambda\neq\mu$, then $\wb^*\vb = \inner{\vb}{\wb} = 0$, and so
  the eigenvectors are orthogonal. \qed
\end{proof}

\begin{theorem}
  Let $\As$ be a Hermitian matrix. If it is diagonalisable, then the similarity
  matrix is unitary, i.e. $\As = \Ps \Ds \Ps^{-1} = \Ps \Ds \Ps^*$.
\end{theorem}

\begin{proof}
  Let $\lambda_1$ be an eigenvalue of $\As$ with eigenvector $\vb$. Let $U =
  \mbox{span}\{\vb\}$. For $\wb \in U^\perp$, then
  \begin{equation*}
    \inner{\vb}{\As \wb} = \inner{\As \vb}{\wb} = \inner{\lambda_1\vb}{\wb} = \lambda_1\inner{\vb}{\wb} = 0,
  \end{equation*}
  hence $\As \wb \in U^\perp$. Supposing $\|\vb\|=1$, we choose an orthonormal
  basis $\ub_2, \ldots \ub_n$ for $U^\perp$, which implies that $\{\vb, \ub_2,
  \ldots \ub_n\}$ is an orthonormal basis for $\mathbb{C}^n$. 
  
  Let $\Ps_1$ be the matrix whose columns are those vectors, which by
  construction means $\Ps^* \Ps = \Is$ by orthogonality. Let $\{\eb_i\}$ be the
  standard basis of $\mathbb{C}^n$, then
  \begin{equation*}
    \Ps_1^* \As \Ps_1 \eb_1 = \Ps_1^* \As \vb = \Ps_1^* \lambda_1 \vb = \lambda_1 \Ps_1^* \vb = \lambda_1 \eb_1 \quad \Rightarrow \quad \Ps_1^* \As \Ps_1 = \begin{pmatrix}\lambda_1 & 0 \\ 0 & \As_1\end{pmatrix}.
  \end{equation*}
  The procedure can be continued for the next one, so that
  \begin{equation*}
    \Ps_2^* \Ps_1^* \As \Ps_1 \Ps_2 = \begin{pmatrix}\lambda_1 & 0 & 0 \\ 0 & \lambda_2 & 0 \\ 0 & 0 & \As_2\end{pmatrix},
  \end{equation*}
  and so we can define $\mathsf{P} = \prod_{i=1}^n \Ps_i$ as the transformation
  matrix. Since each $\Ps_i$ is unitary, its product is unitary. \qed
\end{proof}

%-------------------------------------------------------------------------------

\section{Diagonalising quadratic forms}

%-------------------------------------------------------------------------------

\section{Symmetric and self-adjoint maps}

%-------------------------------------------------------------------------------

\section{Reflections}

%-------------------------------------------------------------------------------

\section{The operator $\mathrm{d}/\mathrm{d}x^2$}

%-------------------------------------------------------------------------------

\section{Jordan normal form}

%===============================================================================

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% r.5 contents
%\tableofcontents

%\listoffigures

%\listoftables

% r.7 dedication
%\cleardoublepage
%~\vfill
%\begin{doublespace}
%\noindent\fontsize{18}{22}\selectfont\itshape
%\nohyphenation
%Dedicated to those who appreciate \LaTeX{} 
%and the work of \mbox{Edward R.~Tufte} 
%and \mbox{Donald E.~Knuth}.
%\end{doublespace}
%\vfill

% r.9 introduction
% \cleardoublepage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% actual useful crap (normal chapters)
\mainmatter

%\part{Basics (?)}


%\backmatter

%\bibliography{refs}
\bibliographystyle{plainnat}

%\printindex

\end{document}

