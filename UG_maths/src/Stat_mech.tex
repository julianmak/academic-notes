\documentclass[letter-paper]{tufte-book}

%%
% Book metadata
\title{Statistical Mechanics}
\author[]{Inusuke Shibemoto}
%\publisher{Research Institute of Valinor}

%%
% If they're installed, use Bergamo and Chantilly from www.fontsite.com.
% They're clones of Bembo and Gill Sans, respectively.
\IfFileExists{bergamo.sty}{\usepackage[osf]{bergamo}}{}% Bembo
\IfFileExists{chantill.sty}{\usepackage{chantill}}{}% Gill Sans

%\usepackage{microtype}
\usepackage{amssymb}
\usepackage{amsmath}
%%
% For nicely typeset tabular material
\usepackage{booktabs}

%% overunder braces
\usepackage{oubraces}

%% 
\usepackage{xcolor}
\usepackage{tcolorbox}

\newtcolorbox[auto counter,number within=section]{derivbox}[2][]{colback=TealBlue!5!white,colframe=TealBlue,title=Box \thetcbcounter:\ #2,#1}                                                          

\makeatletter
\@openrightfalse
\makeatother

%%
% For graphics / images
\usepackage{graphicx}
\setkeys{Gin}{width=\linewidth,totalheight=\textheight,keepaspectratio}
\graphicspath{{figs/}}

% The fancyvrb package lets us customize the formatting of verbatim
% environments.  We use a slightly smaller font.
\usepackage{fancyvrb}
\fvset{fontsize=\normalsize}

\usepackage[plain]{fancyref}
\newcommand*{\fancyrefboxlabelprefix}{box}
\fancyrefaddcaptions{english}{%
  \providecommand*{\frefboxname}{Box}%
  \providecommand*{\Frefboxname}{Box}%
}
\frefformat{plain}{\fancyrefboxlabelprefix}{\frefboxname\fancyrefdefaultspacing#1}
\Frefformat{plain}{\fancyrefboxlabelprefix}{\Frefboxname\fancyrefdefaultspacing#1}

%%
% Prints argument within hanging parentheses (i.e., parentheses that take
% up no horizontal space).  Useful in tabular environments.
\newcommand{\hangp}[1]{\makebox[0pt][r]{(}#1\makebox[0pt][l]{)}}

%% 
% Prints an asterisk that takes up no horizontal space.
% Useful in tabular environments.
\newcommand{\hangstar}{\makebox[0pt][l]{*}}

%%
% Prints a trailing space in a smart way.
\usepackage{xspace}
\usepackage{xstring}

%%
% Some shortcuts for Tufte's book titles.  The lowercase commands will
% produce the initials of the book title in italics.  The all-caps commands
% will print out the full title of the book in italics.
\newcommand{\vdqi}{\textit{VDQI}\xspace}
\newcommand{\ei}{\textit{EI}\xspace}
\newcommand{\ve}{\textit{VE}\xspace}
\newcommand{\be}{\textit{BE}\xspace}
\newcommand{\VDQI}{\textit{The Visual Display of Quantitative Information}\xspace}
\newcommand{\EI}{\textit{Envisioning Information}\xspace}
\newcommand{\VE}{\textit{Visual Explanations}\xspace}
\newcommand{\BE}{\textit{Beautiful Evidence}\xspace}

\newcommand{\TL}{Tufte-\LaTeX\xspace}

% Prints the month name (e.g., January) and the year (e.g., 2008)
\newcommand{\monthyear}{%
  \ifcase\month\or January\or February\or March\or April\or May\or June\or
  July\or August\or September\or October\or November\or
  December\fi\space\number\year
}


\newcommand{\urlwhitespacereplace}[1]{\StrSubstitute{#1}{ }{_}[\wpLink]}

\newcommand{\wikipedialink}[1]{http://en.wikipedia.org/wiki/#1}% needs \wpLink now

\newcommand{\anonymouswikipedialink}[1]{\urlwhitespacereplace{#1}\href{\wikipedialink{\wpLink}}{Wikipedia}}

\newcommand{\Wikiref}[1]{\urlwhitespacereplace{#1}\href{\wikipedialink{\wpLink}}{#1}}

% Prints an epigraph and speaker in sans serif, all-caps type.
\newcommand{\openepigraph}[2]{%
  %\sffamily\fontsize{14}{16}\selectfont
  \begin{fullwidth}
  \sffamily\large
  \begin{doublespace}
  \noindent\allcaps{#1}\\% epigraph
  \noindent\allcaps{#2}% author
  \end{doublespace}
  \end{fullwidth}
}

% Inserts a blank page
\newcommand{\blankpage}{\newpage\hbox{}\thispagestyle{empty}\newpage}

\usepackage{units}

% Typesets the font size, leading, and measure in the form of 10/12x26 pc.
\newcommand{\measure}[3]{#1/#2$\times$\unit[#3]{pc}}

% Macros for typesetting the documentation
\newcommand{\hlred}[1]{\textcolor{Maroon}{#1}}% prints in red
\newcommand{\hangleft}[1]{\makebox[0pt][r]{#1}}
\newcommand{\hairsp}{\hspace{1pt}}% hair space
\newcommand{\hquad}{\hskip0.5em\relax}% half quad space
\newcommand{\TODO}{\textcolor{red}{\bf TODO!}\xspace}
\newcommand{\na}{\quad--}% used in tables for N/A cells
\providecommand{\XeLaTeX}{X\lower.5ex\hbox{\kern-0.15em\reflectbox{E}}\kern-0.1em\LaTeX}
\newcommand{\tXeLaTeX}{\XeLaTeX\index{XeLaTeX@\protect\XeLaTeX}}
% \index{\texttt{\textbackslash xyz}@\hangleft{\texttt{\textbackslash}}\texttt{xyz}}
\newcommand{\tuftebs}{\symbol{'134}}% a backslash in tt type in OT1/T1
\newcommand{\doccmdnoindex}[2][]{\texttt{\tuftebs#2}}% command name -- adds backslash automatically (and doesn't add cmd to the index)
\newcommand{\doccmddef}[2][]{%
  \hlred{\texttt{\tuftebs#2}}\label{cmd:#2}%
  \ifthenelse{\isempty{#1}}%
    {% add the command to the index
      \index{#2 command@\protect\hangleft{\texttt{\tuftebs}}\texttt{#2}}% command name
    }%
    {% add the command and package to the index
      \index{#2 command@\protect\hangleft{\texttt{\tuftebs}}\texttt{#2} (\texttt{#1} package)}% command name
      \index{#1 package@\texttt{#1} package}\index{packages!#1@\texttt{#1}}% package name
    }%
}% command name -- adds backslash automatically
\newcommand{\doccmd}[2][]{%
  \texttt{\tuftebs#2}%
  \ifthenelse{\isempty{#1}}%
    {% add the command to the index
      \index{#2 command@\protect\hangleft{\texttt{\tuftebs}}\texttt{#2}}% command name
    }%
    {% add the command and package to the index
      \index{#2 command@\protect\hangleft{\texttt{\tuftebs}}\texttt{#2} (\texttt{#1} package)}% command name
      \index{#1 package@\texttt{#1} package}\index{packages!#1@\texttt{#1}}% package name
    }%
}% command name -- adds backslash automatically
\newcommand{\docopt}[1]{\ensuremath{\langle}\textrm{\textit{#1}}\ensuremath{\rangle}}% optional command argument
\newcommand{\docarg}[1]{\textrm{\textit{#1}}}% (required) command argument
\newenvironment{docspec}{\begin{quotation}\ttfamily\parskip0pt\parindent0pt\ignorespaces}{\end{quotation}}% command specification environment
\newcommand{\docenv}[1]{\texttt{#1}\index{#1 environment@\texttt{#1} environment}\index{environments!#1@\texttt{#1}}}% environment name
\newcommand{\docenvdef}[1]{\hlred{\texttt{#1}}\label{env:#1}\index{#1 environment@\texttt{#1} environment}\index{environments!#1@\texttt{#1}}}% environment name
\newcommand{\docpkg}[1]{\texttt{#1}\index{#1 package@\texttt{#1} package}\index{packages!#1@\texttt{#1}}}% package name
\newcommand{\doccls}[1]{\texttt{#1}}% document class name
\newcommand{\docclsopt}[1]{\texttt{#1}\index{#1 class option@\texttt{#1} class option}\index{class options!#1@\texttt{#1}}}% document class option name
\newcommand{\docclsoptdef}[1]{\hlred{\texttt{#1}}\label{clsopt:#1}\index{#1 class option@\texttt{#1} class option}\index{class options!#1@\texttt{#1}}}% document class option name defined
\newcommand{\docmsg}[2]{\bigskip\begin{fullwidth}\noindent\ttfamily#1\end{fullwidth}\medskip\par\noindent#2}
\newcommand{\docfilehook}[2]{\texttt{#1}\index{file hooks!#2}\index{#1@\texttt{#1}}}
\newcommand{\doccounter}[1]{\texttt{#1}\index{#1 counter@\texttt{#1} counter}}

\newcommand{\studyq}[1]{\marginnote{Q: #1}}

\hypersetup{colorlinks}% uncomment this line if you prefer colored hyperlinks (e.g., for onscreen viewing)

% Generates the index
\usepackage{makeidx}
\makeindex

\setcounter{tocdepth}{3}
\setcounter{secnumdepth}{3}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% custom commands

\newtheorem{theorem}{\color{pastel-blue}Theorem}[section]
\newtheorem{lemma}[theorem]{\color{pastel-blue}Lemma}
\newtheorem{proposition}[theorem]{\color{pastel-blue}Proposition}
\newtheorem{corollary}[theorem]{\color{pastel-blue}Corollary}

\newenvironment{proof}[1][Proof]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{example}[1][Example]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

\hyphenpenalty=5000

% more pastel ones
\xdefinecolor{pastel-red}{rgb}{0.77,0.31,0.32}
\xdefinecolor{pastel-green}{rgb}{0.33,0.66,0.41}
\definecolor{pastel-blue}{rgb}{0.30,0.45,0.69} % crayola blue
\definecolor{gray}{rgb}{0.2,0.2,0.2} % dark gray

\xdefinecolor{orange}{rgb}{1,0.45,0}
\xdefinecolor{green}{rgb}{0,0.35,0}
\definecolor{blue}{rgb}{0.12,0.46,0.99} % crayola blue
\definecolor{gray}{rgb}{0.2,0.2,0.2} % dark gray

\xdefinecolor{cerulean}{rgb}{0.01,0.48,0.65}
\xdefinecolor{ust-blue}{rgb}{0,0.20,0.47}
\xdefinecolor{ust-mustard}{rgb}{0.67,0.52,0.13}

%\newcommand\comment[1]{{\color{red}#1}}

\newcommand{\dy}{\partial}
\newcommand{\ddy}[2]{\frac{\dy#1}{\dy#2}}

\newcommand{\ab}{\boldsymbol{a}}
\newcommand{\bb}{\boldsymbol{b}}
\newcommand{\cb}{\boldsymbol{c}}
\newcommand{\db}{\boldsymbol{d}}
\newcommand{\eb}{\boldsymbol{e}}
\newcommand{\lb}{\boldsymbol{l}}
\newcommand{\nb}{\boldsymbol{n}}
\newcommand{\tb}{\boldsymbol{t}}
\newcommand{\ub}{\boldsymbol{u}}
\newcommand{\vb}{\boldsymbol{v}}
\newcommand{\xb}{\boldsymbol{x}}
\newcommand{\wb}{\boldsymbol{w}}
\newcommand{\yb}{\boldsymbol{y}}

\newcommand{\Xb}{\boldsymbol{X}}

\newcommand{\ex}{\mathrm{e}}
\newcommand{\zi}{{\rm i}}

\newcommand\Real{\mbox{Re}} % cf plain TeX's \Re and Reynolds number
\newcommand\Imag{\mbox{Im}} % cf plain TeX's \Im

\newcommand{\zbar}{{\overline{z}}}

\newcommand\Def[1]{\textbf{#1}}

\newcommand{\qed}{\hfill$\blacksquare$}
\newcommand{\qedwhite}{\hfill \ensuremath{\Box}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% some extra formatting (hacked from Patrick Farrell's notes)
%  https://courses.maths.ox.ac.uk/node/view_material/4915
%

% chapter format
\titleformat{\chapter}%
  {\huge\rmfamily\itshape\color{pastel-red}}% format applied to label+text
  {\llap{\colorbox{pastel-red}{\parbox{1.5cm}{\hfill\itshape\huge\color{white}\thechapter}}}}% label
  {1em}% horizontal separation between label and title body
  {}% before the title body
  []% after the title body

% section format
\titleformat{\section}%
  {\normalfont\Large\itshape\color{pastel-green}}% format applied to label+text
  {\llap{\colorbox{pastel-green}{\parbox{1.5cm}{\hfill\color{white}\thesection}}}}% label
  {1em}% horizontal separation between label and title body
  {}% before the title body
  []% after the title body

% subsection format
\titleformat{\subsection}%
  {\normalfont\large\itshape\color{pastel-blue}}% format applied to label+text
  {\llap{\colorbox{pastel-blue}{\parbox{1.5cm}{\hfill\color{white}\thesubsection}}}}% label
  {1em}% horizontal separation between label and title body
  {}% before the title body
  []% after the title body

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

% Front matter
%\frontmatter

% r.3 full title page
%\maketitle

% v.4 copyright page

\chapter*{}

\begin{fullwidth}

\par \begin{center}{\Huge Statistical Mechanics 3/4H}\end{center}

\vspace*{5mm}

\par \begin{center}{\Large typed up by B. S. H. Mithrandir}\end{center}

\vspace*{5mm}

\begin{itemize}
  \item \textit{Last compiled: \monthyear}
  \item Adapted from notes of Veronika Hubeny and someone else (maybe Mukund?), Durham
  \item This was part of the Durham 3/4H elective. Includes introduction to
  classical thermodynamics and some quantum systems.
  \item It is sometimes said that there are probably two ways to approach
  thermodynamics: start from classical thermodynamics first, or start from a
  stat mech approach. These notes take the latter approach. From a physical
  understanding point of view, these particular notes might be more informative
  / useful if the intended user already has some exposure already to the
  traditional way of approaching thermodynamics.
  \item[]
  \item \TODO diagrams
  \item \TODO probably wants more context...
  \item \TODO to possibly merge in some other notes (e.g. expand a bit more on
  classical thermodynamics; or maybe not and keep it separate to highlight
  differences in approach)
\end{itemize}

\par

\par Licensed under the Apache License, Version 2.0 (the ``License''); you may not
use this file except in compliance with the License. You may obtain a copy
of the License at \url{http://www.apache.org/licenses/LICENSE-2.0}. Unless
required by applicable law or agreed to in writing, software distributed
under the License is distributed on an \smallcaps{``AS IS'' BASIS, WITHOUT
WARRANTIES OR CONDITIONS OF ANY KIND}, either express or implied. See the
License for the specific language governing permissions and limitations
under the License.
\end{fullwidth}

%===============================================================================

\chapter{Classical thermodynamics}

The aim is to describe thermodynamics in \Def{equilibrium}.

%-------------------------------------------------------------------------------

\subsection{Postulates and definitions}

\Def{Extensive variables} $X_i$ of a thermodynamic system are those that depend
and scales proportionally to the system size. The set of all extensive variables
are sufficient to describe the system in equilibrium. Examples include
\begin{itemize}
  \item energy
  \item volume
  \item charge
  \item number of particles
\end{itemize}

Thermodynamic system can either be \Def{isolated} or \Def{interacting} with an
external environment. Isolated systems to not exchange any of the extensive
parameters the environment.

\vspace*{3mm}

\Def{Thermodynamic postulate 1}:

\begin{center}\textit{There exists and equilibrium state.}\end{center}

\begin{proposition}[Generalised first law of thermodynamics]
At equilibrium,
\begin{equation}
  \frac{\mathrm{d}X_i}{\mathrm{d}t} = 0
\end{equation}
for all extensive variables $X_i$ in a system.
\end{proposition}

The above is usually phrased as\sidenote{Sign of $W$ depends on convention.
Positive sign here as energy transferring into system.}
\begin{equation}
  \delta U = \delta Q + \delta W,
\end{equation}
where $U$ is the \Def{internal energy}, $Q$ is the \Def{heat}, and $W$ is
\Def{work done}. Ther conservation of internal energy gives
\begin{equation}
  \mathrm{d}U = T\ \mathrm{d}S - P\ \mathrm{d}V + \phi_i q_i,
\end{equation}
where $S$ is the \Def{entropy} (see later), $T$ is temperature, $P$ is
\Def{pressure}, $V$ is \Def{volume}, and $\phi_i$ and $q_i$ denote some other
form of change. The exact definitions of some of these variables with respect to
the more familiar thermodynamic quantities will be provided later.

\vspace*{3mm}

\Def{Thermodynamic postulate 2}:

\begin{center}\textit{There exists a function $S$ called the \Def{entropy} for all thermodynamic systems such that}\end{center}
\textit{
\begin{enumerate}
  \item $S = S(X)$, $X = \{X_i\}$, and $S$ is itself extensive;
  \item $S \in C^{\infty}$ (usually)
  \item $S$ is maximised as a function of $X$ when the system is in thermal
  equilibrium.
\end{enumerate}
}

The two postulates constrain the allowed physical processes that go on within a
thermodynamic system.

\begin{example}
\marginnote{Superscripts denote system, subscripts denote variable within a system.}
Given two systems $\mathcal{S}^{1,2}$ with extensive parameters $X^{1,2}$
respectively, for $\mathcal{S} = \mathcal{S}^1 \cup \mathcal{S}^2$, the total
set of extensive parameters is $X = X^1 + X^2$. Since entropy is maximised,
$S(X) = S(X^1 + X^2) \geq S_1(X^1) + S_2(X^2)$.

Suppose $S(X) = S_1(X^1 + \delta X^1) + S_2(X^2 + \delta X^2)$, where $\delta
X^2$ is the required change to bring the system total system to equilibrium.
Then we want $\delta X^! + \delta X^2 = 0$. Considering the variation, we have
\begin{equation*}
  \delta S = \frac{\partial S_1}{\partial X^1} \delta X^1 + \frac{\partial S_2}{\partial X^2} \delta X^2 = \delta X^1 \left(\frac{\partial S_1}{\partial X^1} - \frac{\partial S_2}{\partial X^2}\right).
\end{equation*}
Since $\delta S \geq 0$, the sign of $\delta X^1$ is the same as $(\partial S_1
/ \partial X^1 - \partial S_2 / \partial X^2)$, thus determining the direction
of flow for $X$.
\end{example}

Some properties of entropy:
\begin{itemize}
  \item $S(X)$ is a homogeneous function of degree 1, i.e. $S(\alpha X) =
  \alpha S(X)$ for some scalar $\alpha$;
  \item the defining relation of entropy in terms of other extensive parameters
  is called the \Def{entropy fundamental relation}, which provides a complete
  specific formulation of the thermodynamic system;
  \item physically, the entropy measures the amount of disorder in the system.
\end{itemize}

\Def{Intensive parameters} $Z_i$ are those that are \Def{conjugate} to the
extensive parameters $X_i$, i.e.
\begin{equation}
  Z_i = \frac{\partial S}{\partial X_i}.
\end{equation}
Clearly $Z_i$ has to be homogeneous functions of degree 0, and $Z_i(X)$ is
called the \Def{equation of state}, but requires additional information in order
to have a complete specification of the thermodynamic system.

\begin{lemma}
If two systems $\mathcal{S}^{1,2}$ interact and exchange extensive variables
$X^{1,2}$, at equilibrium, we have $Z^1 = Z^2$.
\end{lemma}
\begin{proof}
Since we are at equilibrium, we aim to maximise $S = S^1 + S^2$ keeping $X = X^1
+ X^2$ fixed. This can be done either by variation, with
\begin{equation*}
  0 = \delta S = \delta X^1 \left(\frac{\partial S_1}{\partial X^1} - \frac{\partial S_2}{\partial X^2}\right) = (Z^1 - Z^2) \delta X^1 \qquad \Rightarrow \qquad Z^1 = Z^2,
\end{equation*}
or by the method of Lagrange multiplies, we aim to maximise $S - \lambda(X^1 +
X^2)$ to get
\begin{equation*}
  \frac{\partial S}{\partial X^1} - \lambda = 0, \qquad \frac{\partial S}{\partial X^2} - \lambda = 0
\end{equation*}
which implies $\lambda = Z^1 = Z^2$.
\qed
\end{proof}

\vspace*{3mm}

\Def{Zeroth law of thermodynamics}\sidenote{More commonly, if two systems are in
thermal equilibrium with a third system, then they are in equlibrium with each
other. This a statement about \emph{transitivity} and establishes thermal
equilibrium as an equivalence relation from a mathematical point of view.}:

\begin{center}\textit{For two systems in thermal equilibrium with each other,
$Z^1_i = Z^2_i$ for all $i$.}\end{center}

When two systems $\mathcal{S}^{1,2}$ are not in equilibrium and $Z^1 > Z^2$,
$\mathrm{d}S > 0$ implies $X$ flows from $\mathcal{S}^1$ to $\mathcal{S}^2$, and
we have a \Def{directed flow}.

Instead of entropy $S$, we can take the internal energy $U$ as the fundamental
variable\sidenote{And really is what would be the more traditional way of
introducing classical thermodynamics.}. We have the \Def{energetic fundamental
relations} with intensive parameters
\begin{equation}
  Y_i = \frac{\partial U}{\partial X_i}.
\end{equation}
From this, we note that
\begin{itemize}
  \item \Def{temperature} $T$ is conjugate to entropy $S$, i.e.
  \begin{equation}
    T = \frac{\partial U}{\partial S};
  \end{equation}
  \item \Def{pressure} $P$ is conjugate to volume $V$, i.e.
  \begin{equation}
    P = -\frac{\partial U}{\partial V};
  \end{equation}
  \item \Def{chemical potential} $\mu$ is conjugate to particle number $N$, i.e.
  \begin{equation}
    \mu = \frac{\partial U}{\partial N}.
  \end{equation}
\end{itemize}
All signs are by convention. Occasionally, we employ the \Def{inverse
temperature}\sidenote{Sometimes this is defined with a factor of the Boltzmann
constant $k_B$, i.e. $\beta = (k_B T)^{-1}$.} variable $\beta$ given by
\begin{equation}
  \beta = \frac{1}{T} = \frac{\partial S}{\partial U}.
\end{equation}

\begin{example}
An alternative phrasing of the first law of thermodynamics\sidenote{Actually the
more traditional way of stating the first law.} is given as energy conservation
(Einstein summation convention implied)
\begin{equation}
  \mathrm{d}U = T\ \mathrm{d}S + \frac{\partial U}{\partial X_i}\ \mathrm{d}X_i = T\ \mathrm{d}S + Y_i\ \mathrm{d}X_i.
\end{equation}
Then, by rephrasing in the conjugate variables, we have
\begin{align*}
  \mathrm{d}S &= \beta\ \mathrm{d}U + Z_i\ \mathrm{d}X_i \\
    &= \beta(T\ \mathrm{d}S + Y_i\ \mathrm{d}X_i) + Z_i\ \mathrm{d}X_i \\
    &= \mathrm{d}S + (\beta Y_i + Z_i)\ \mathrm{d}X_i,
\end{align*}
implying
\begin{equation}
  Y_i = -T Z_i,
\end{equation}
i.e. all individual intensive parameters associated with $T$ and $S$ are related
by a factor of temperature.
\end{example}

A system is said to undergo a \Def{quasi-static} process if, at every instant of
time along the path in the phase space, one can approximate the system as being
in equilibrium. A quasi-static process is \Def{adiabatic} if the system does not
exchange heat with its surroundings, i.e. $\mathrm{d}S = 0$ along the
appropriate curve in phase space.

%-------------------------------------------------------------------------------

\subsection{Some examples}

\begin{enumerate}
  \item Consider an \Def{ideal gas}\sidenote{Randomly moving point particles
  that are not subject to inter-particle interactions.} in equilibrium, which is
  made to undergo a quasi-static expansion from $V_i$ to $V_f$ adiabatically.
  During this process, $PV^\gamma = \textnormal{constant}$, where $\gamma$ is
  some number (the \Def{adaibatic index} or the \Def{heat capacity ratio}). We
  aim to calculate the change in internal energy of the gas, and the work done
  and heat absorbed by the gas when it goes from $(P_i, V_i)$ to $(P_f, V_f)$.

  Starting from the first law with $\mathrm{d}U = T\ \mathrm{d}S - P\
  \mathrm{d}V$, since the process is adiabatic, $\mathrm{d}S = 0$, so $\mathrm{d}U
  = - P\ \mathrm{d}V$. By assumption,
  \begin{equation*}
    PV^\gamma = P_i V_i^\gamma = C \qquad \Rightarrow \qquad P = \frac{P_i
    V_i^\gamma}{V^\gamma},
  \end{equation*}
  so integrating gives
  \begin{align*}
    \Delta U = U_f - U_i &= \int_{V_i}^{V_f} - \frac{P_i V_i^\gamma}{V^\gamma}\; \mathrm{d}V \\
      &= -P_i V_i^\gamma \left[\frac{V^{-\gamma + 1}}{-\gamma + 1}\right]^{V_f}_{V_i}\\
      &= \frac{P_f V_f - P_i V_i}{\gamma - 1}
  \end{align*}
  via using $P_i V_i^\gamma = P_f V_f^\gamma$.

  \item From $S \sim (UVN)^{1/3}$, derive the equation of state.

  This is asking for the intensive variables and their relations to the
  extensive variables. For some constant $c$, we have $U = cS^3 / (VN)$, so
  \begin{equation*}
    T = \frac{\partial U}{\partial S} = \frac{3cS^2}{VN}, \quad
    P = -\frac{\partial U}{\partial V} = \frac{cS^3}{V^2 N}, \quad
    \mu = \frac{\partial U}{\partial N} = -\frac{cS^3}{VN^2}.
  \end{equation*}

  \item Derive the \Def{Euler relation}
  \begin{equation}
    U(S, X_i) = TS + X_i Y_i,
  \end{equation}
  where $Y_i$ are the intensive variables associated with $X_i$ via the internal
  energy (instead of a entropy).

  Since $U$ is extensive, we have $U(\lambda S, \lambda X_i) = \lambda U(S,
  X_i)$ by homogeneity, so by chain rule and taking a derivative with respect to
  $\lambda$,
  \begin{equation*}
    \frac{\partial U}{\partial (\lambda S)}(\lambda S, \lambda X_i) \frac{\partial (\lambda S)}{\partial \lambda} + \frac{\partial U}{\partial (\lambda X_i)}(\lambda S, \lambda X_i) \frac{\partial (\lambda X_i)}{\partial \lambda} = U(S, X_i).
  \end{equation*}
  If we set $\lambda = 1$, then the result follows on noting $\partial U /
  \partial S = T$, and $\partial U / \partial X_i = Y_i$.

  \item Prove the \Def{Gibbs--Duhem relation}
  \begin{equation}
    S\ \mathrm{d}T + X_i\ \mathrm{d}Y_i = 0.
  \end{equation}

  Differentiating the Euler relation leads to
  \begin{equation*}
    \mathrm{d}U = T\ \mathrm{d}S + S\ \mathrm{d}T + X_i\ \mathrm{d}Y_i + Y_i\ \mathrm{d}X_i,
  \end{equation*}
  and since the first law tells us $\mathrm{d}U = T\ \mathrm{d}S + Y_i\
\mathrm{d}X_i$, the result follows.

  \item Show that the Euler and Gibbs--Duhem relations can be written as
  \begin{equation*}
    S = \frac{1}{T} U + Z_i X_i, \qquad U\ \mathrm{d}\left(\frac{1}{T}\right) + X_i\ \mathrm{d}Z_i = 0,
  \end{equation*}
  where $Z_i$ is the intensive variables associated with the entropy.
  
  Recall that $Y_i = -T Z_i$ and $\mathrm{d}U = T\ \mathrm{d}S + Y_i\
  \mathrm{d}X_i$, which implies the first relation via substitution and
  integration. Differentiating the first relation leads to
  \begin{equation*}
    \mathrm{d}S = U\ \mathrm{d}\left(\frac{1}{T}\right) + \frac{1}{T}\ \mathrm{d}U + X_i\ \mathrm{d}Z_i + Z_i\ \mathrm{d}Z_i,
  \end{equation*}
  and upon using the first law by substituting $\mathrm{d}S$ leads to the second
  relation.
  
  \item For an ideal gas, given $PV = nRT$ and $U = (3/2)RNT$, where $R$ is the
  universal gas constant, derive the associated \Def{entropic fundamental
  relation} $S(U,V,N)$.

  To make the equation more exact, consider scaling by $N$ as $u = U/N$, $v =
  V/N$, which leads to
  \begin{equation*}
    Pv = RT, \qquad u = \frac{3}{2}RT.
  \end{equation*}
  Note that we have
  \begin{itemize}
    \item $X_i$: $V$, $N$;
    \item $Y_i$: $-P$, $\mu$,
    \item $Z_i$: $P/T$, $-\mu / T$.
  \end{itemize}
  Using the Gibbs--Duhem relation and scaling by $N$ leads to
  \begin{equation*}
    u\ \mathrm{d}\left(\frac{1}{T}\right) + v\ \mathrm{d}\left(\frac{P}{T}\right) = \mathrm{d}\left(\frac{\mu}{T}\right).
  \end{equation*}
  By computing the relevant derivatives, we have
  \begin{equation*}
    \mathrm{d}\left(\frac{\mu}{T}\right) = -\frac{3R}{2}\frac{\mathrm{d}u}{u} - R\frac{\mathrm{d}v}{v},
  \end{equation*}
  which upon integrating leads to
  \begin{equation*}
    \frac{\mu}{T} = \frac{3R}{2} \log\frac{u}{u_0} - R \log\frac{v}{v_0}.
  \end{equation*}
  Using Euler's relation in the form
  \begin{equation*}
    S = \frac{1}{T}U + \frac{P}{T}V - \frac{\mu}{T}N,
  \end{equation*}
  substituting accordingly results in
  \begin{equation*}
    S(U,V,N) = \frac{5}{2}NR + NR \log\left(\frac{u^{3/2}v}{u_0^{3/2}v_0}\right).
  \end{equation*}
\end{enumerate}

%-------------------------------------------------------------------------------

\subsection{Thermodynamic potentials}

Recall that the Legendre transform of a given function $f(x,y)$ is achieved via
the consideration of
\begin{equation*}
  \mathrm{d}f = \frac{\partial f}{\partial x}\mathrm{d}x + \frac{\partial f}{\partial y}\mathrm{d}y \equiv u(x,y)\ \mathrm{d}x + v(x,y)\ \mathrm{d}y.
\end{equation*}
To define $f$ as a function of $u$ rather than $x$, consider
\begin{equation*}
  g(v,y) = f - ux,
\end{equation*}
which then implies
\begin{equation*}
  \mathrm{d}y = \mathrm{d}f - u\ \mathrm{d}x - x\ \mathrm{d}u = v\ \mathrm{d}y - x\ \mathrm{d}u,
\end{equation*}
where $v = \partial g / \partial y$ and $x - \partial g / \partial u$.

The Legendre transform is an involute transformation of real valued convex
functions in one dimension, such that their derivatives of the original and
transformed functions are inverses of each other. Physically is used to convert
functions of one quantity to a function of its conjugate variable (e.g. position
to momenta, pressure to volume, temperature to entropy, and vice-versa). The
\Def{thermodynamic potentials} are Legendre transforms of $U$ with respect to
the various extensive parameters. Some important ones are:
\begin{itemize}
  \item \textbf{Helmholtz free energy}, which is the transformation of $U$
  with respect to $S$, given by
  \begin{equation}
    F(T, X_i) = U - T.
  \end{equation}
  
  \item \textbf{Enthalpy}, which is the transformation of $U$
  with respect to $V$, given by
  \begin{equation}
    H(P, S, X_i) = U + PV.
  \end{equation}
  
  \item \textbf{Gibbs free energy}, which is the transformation of $U$
  with respect to $S$ and $V$, given by
  \begin{equation}
    G(T, X_i) = U - TS + PV.
  \end{equation}
\end{itemize}
\marginnote{These could be thought of as the capacity to do mechanical and
non-mechanical work ($F$), to do non-mechanical work and release heat ($H$),
capacity to do non-mechanical (reversible) work ($G$), and capacity to do work
and release heat $U$.} Sometimes it is easier to consider these potentials
instead of directly trying to find the measurements for the extensive
parameters. Which one we choose depends on how the system is described. For
example, if it is natural to vary $T$ for example, then we consider $F$.

\begin{example}
Show that $\mathrm{d}F = -S\ \mathrm{d}T + Y_i\ \mathrm{d}X_i$.

From the definition of the Helmholtz free energy,
\begin{align*}
  \mathrm{d}F &= \mathrm{d}U - T\ \mathrm{d}S - S\ \mathrm{d}T\\
    &= (T\ \mathrm{d}S + Y_i\ \mathrm{d}X_i) - T\ \mathrm{d}S - S\ \mathrm{d}T\\
    &= -S\ \mathrm{d}T + Y_i\ \mathrm{d}X_i.
\end{align*}

Since $\mathrm{d}F = (\partial F / \partial T)\ \mathrm{d}T + (\partial F /
\partial X_i)\ \mathrm{d}X_i$, we note that
\begin{equation}
  -S(T, X_i) = \frac{\partial F}{\partial T}, \qquad Y_i(T, X_i) = \frac{\partial F}{\partial X_i},
\end{equation}
and similar identities hold for the other thermodynamic potentials, i.e., from
the thermodynamic potentials you can derive essentially all the things you want
to know.
\end{example}

From the thermodynamic potentials, one can derive the \Def{Maxwell relations},
which essentially relate the second derivatives of the potentials. For
simplicity, we concentrate on a system with extensive parameters $U$, $S$, $V$
and $N$. Recall that $\mathrm{d}U = T\ \mathrm{d}S - P\ \mathrm{d}V + \mu\
\mathrm{d}N$, so that
\begin{equation*}
  T = \left.\frac{\partial U}{\partial S}\right|_{V, N}, \qquad
  P = -\left.\frac{\partial U}{\partial V}\right|_{S, N}, \qquad
  \mu = \left.\frac{\partial U}{\partial N}\right|_{S, V}.
\end{equation*}
Assuming we have the relevant conditions required for partial derivatives to
commute, then we would have for example
\begin{equation*}
  \left.\frac{\partial T}{\partial V}\right|_N = \frac{\partial}{\partial V}\left.\frac{\partial U}{\partial S}\right|_N = \frac{\partial}{\partial S}\left.\frac{\partial U}{\partial V}\right|_N = -\left.\frac{\partial P}{\partial S}\right|_N,
\end{equation*}
and many others. These can also be derived from $F$, $H$ and $G$ via
\begin{align*}
  \mathrm{d}F &= -S\ \mathrm{d}T - P\ \mathrm{d}V + \mu\ \mathrm{d}N,\\
  \mathrm{d}H &= +T\ \mathrm{d}S + V\ \mathrm{d}P + \mu\ \mathrm{d}N,\\
  \mathrm{d}G &= -S\ \mathrm{d}T + V\ \mathrm{d}P + \mu\ \mathrm{d}N.
\end{align*}

In general, Maxwell relatives are reflective of the symmetry properties of the
physical quantities. We have
\begin{equation}
  \frac{\partial Y_i}{\partial X_j} = \frac{\partial X_i}{\partial Y_j}, \qquad
  \frac{\partial Y_i}{\partial Y_j} = -\frac{\partial X_j}{\partial X_i}.
\end{equation}

Some useful physical quantities arising from the thermodynamic potentials are as
follows:
\begin{itemize}
  \item \Def{Specific heat}, defined as
  \begin{equation}
    c = T \frac{\partial S}{\partial T}.
  \end{equation}
  For the specific heats we need to specify which quantities are being held fix.
  For example, if have hold volume fixed ($\mathrm{d}V = 0$), then
  \begin{equation}
    c_V = \left. T\frac{\partial S}{\partial T}\right|_V = \left. -T\frac{\partial^2 F}{\partial T^2}\right|_V.
  \end{equation}
  On the other hand, if we hold pressure fixed ($\mathrm{d}P = 0$), then we have
  \begin{equation}
    c_P = \left. T\frac{\partial S}{\partial T}\right|_P = \left. -T\frac{\partial^2 G}{\partial T^2}\right|_P,
  \end{equation}
  and note the appearance of the Gibbs instead of Helmholtz free energy
  appearing here. Note also that $\gamma = c_P / c_V$.
  
  \item The \Def{susceptibility} (variation of extensive quantity with the
  corresponding intensive quantity) is defined as
  \begin{equation}
    \chi = \frac{\partial X}{\partial Y},
  \end{equation}
  for conjugate $X$ and $Y$.
  
  \item The \Def{compressibility} is defined as
  \begin{equation}
    \kappa = -\frac{1}{V}\frac{\partial V}{\partial P}.
  \end{equation}
  
  \item The \Def{coefficient of thermal expansion} is defined as
  \begin{equation}
    \alpha = \frac{1}{V}\frac{\partial V}{\partial T}.
  \end{equation}
\end{itemize}

%===============================================================================

\chapter{Ensembles}

An \Def{(statistical) ensemble} considers a collection of a large (possibly
infinite) number of virtual copies all at once, each of which represents a
possible state that the real system might be in, and we consider obtaining the
statistics of the ensemble\sidenote{Concept introduced by Gibbs.}. An ensemble
can be thought of as a probability distribution for the state of the system.

%-------------------------------------------------------------------------------

\subsection{Recap on probability}

Recall that, for a random variable $X$, the \Def{$m^{\textnormal{th}}$ moment of
probability} is defined as
\begin{equation}
  \overline{X^m} = \langle X^m \rangle = \begin{cases} \sum_{i=1}^n x_i^m p(x_i), & X \textnormal{ discrete} \\ \int x^m p(x)\; \mathrm{d}x, & X \textnormal{ continuous},\end{cases}
\end{equation}
where $p$ are the relevant probability density functions (pdfs). For example,
the usual mean (expectation) and variance are the first and second central
moments respectively, i.e.
\begin{equation*}
  \mathbb{E}(X) = \mu = \langle X \rangle, \qquad \textnormal{Var}(X) = \sigma^2 = \langle (X - \mathbb{E}(X))^2 \rangle = \langle X^2 \rangle - \langle X \rangle^2.
\end{equation*}

The \Def{moment generating function} is then defined as
\begin{equation}
  M(t) = \int \mathrm{e}^{xt} p(x)\; \mathrm{d}x = \sum_{n=0}^\infty \frac{t^n}{n!}\langle x^n\rangle.
\end{equation}
The moments are then computed by evaluating derivatives of $M(t)$ at $t=0$
accordingly, i.e. $\langle x^n\rangle = M^{(n)}(0)$.

A \Def{Bernoulli random variable} $X$ is one that can only take two different
values, and thus follow the \Def{binomial distribution}
\begin{equation}
  X\sim \mbox{B}(n, p_0), \qquad P(X = k) = \begin{pmatrix}n \\ k\end{pmatrix} p_0^k (1 - p_0)^{n-k},
\end{equation}
for some specified $p_0$. The binomial distribution can be viewed as a Bernoulli
trial repeated $n$ times. The binomial distribution has some special limits:
\begin{itemize}
  \item for $n\gg 1$ and $p_0 \ll 1$ such that $n p_0 = \lambda < \infty$, we
  tend to the \Def{Poisson distribution}
  \begin{equation}
    \frac{\lambda^k \mathrm{e}^{-\lambda}}{k!} = P(X = k), \qquad X \sim \mbox{Po}(\lambda).
  \end{equation}
  
  \item for $n\gg 1$, by the \Def{central limit theorem}, we tend to the
  \Def{Gaussian distribution} 
  \begin{equation}
    \frac{1}{\sqrt{2\pi\sigma^2}}\mathrm{e}^{-(x - \mu)^2 / (2\sigma^2)} = p(x), \qquad X \sim \mbox{N}(\mu, \sigma).
  \end{equation}
\end{itemize}

\begin{theorem}[Central limit theorem]
Let $\{X_k\}$ be a sequence of mutually independent, identically distributed
random variable whose mean and variance are defined as $\mu = \langle X \rangle$
and $\sigma^2 = \langle (X - \mu)^2 \rangle$. If $S_n = \sum X_k$ then, for all
$x \in \mathbb{R}$,
\begin{equation*}
  \lim_{n\to\infty} P\left(\frac{S_n - n\mu}{\sqrt{n\sigma^2}} < X\right) = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^x \mathrm{e}^{-u^2/2}\; \mathrm{d}u,
\end{equation*}
where $u$ is a re-scaled variable. \qedwhite
\end{theorem}

\begin{example}
(\textbf{1d random walk}) At every point on $\mathbb{Z}$, one can move left with
probability $p_L = p$ and to the right with $p_R = 1 - p_L$. Starting from
$x=0$, the probability of finding the object at $x=m\in \mathbb{Z}$ after $N$
steps is given by a binomial distribution. Suppose we move $n_L$ steps left and
a $n_R$ steps right, then we have $n_L + n_R = N$ and $m = n_R - n_L$. Thus the probability of object being at $x=m$ after $N$ steps is given by
\begin{equation*}
  P(X = m) = \begin{pmatrix}N \\ n_R\end{pmatrix} p_R^{n_R} p_L^{n_L} = \begin{pmatrix}N \\ (N+m)/2\end{pmatrix} p^{(N-m)/2} (1-p)^{(N+m)/2}.
\end{equation*}
It is clear that if $p=1/2$, then $\langle X_N\rangle = 0$, otherwise there is a
bias towards one direction.
\end{example}

%-------------------------------------------------------------------------------

\subsection{Recap on classical mechanics}

For a given \Def{macrostate} $(N, V, E)$, at any given time $t$, is equally
likely to be in any one of an extremely large number of distinct
\Def{microstates}, specified by the instantaneous position and momenta of all
particles that constitute the system. As time passes, a system evolves through
distinct microstates, and we aim to obtain the long time behaviour via averaging
the microstates. One possible way is to consider an ensemble of copies
characterised by the same macrostate, but ranging over microstates. Assuming
ergodicity, time averaging corresponds to ensemble averaging, and we can decide
to take one or the other depending on which is more convenient.

Recall that, given a Lagrangian $L(q, \dot{q}; t)$, where $q$ is a generalised
co-ordinate and dot denotes a partial time derivative, the Euler--Lagrange
equations are given by
\begin{equation}
  \frac{\mathrm{d}}{\mathrm{d}t}\frac{\partial L}{\partial \dot{q}_i} - \frac{\partial L}{\partial q_i} = 0,
\end{equation}
which is a set of $n$ second order ODEs that determines all $q_i(t)$ given $2n$
initial conditions. By defining the conjugate momenta variable as $p_i =
\partial L / \partial \dot{q}_i$, we consider the Legendre transform of the
Lagrangian
\begin{equation*}
  H(q, p; t) = \sum_i \dot{q}_i p_i - L(q, \dot{q}; t),
\end{equation*}
where $H$ is the Hamiltonian, which can be interpreted as the total energy in a
system. The Hamilton--Jacobi equations are then
\begin{equation}
  \dot{q}_i = \frac{\partial H}{\partial p_i}, \qquad \dot{p_i} = -\frac{\partial H}{\partial q_i},
\end{equation}
and now we have instead $2n$ first order ODEs. Time evolution of the system is
then defined by the trajectory in the $(q,p)$ phase space.

In general, we want to consider many particles in $d$ dimensions interacting in
a complicated fashion. We will generally be interested in the behaviour of the
system in some narrow range of energies $H(p,q) \in (E-\delta E, E+\delta E)$.
An ensemble will then be represented by a ``swarm'' of representative points
moving on the phase space. We define a \Def{density function} $\rho(p,q;t)$ as
a measure of how the members of the ensemble are distributed over all possible
microstates at different instances of time. For infinitesimal volume
$\mathrm{d}\omega$, $\rho\ \mathrm{d}\omega$ gives the number of representative
points around some specified $(p,q)$. We this, we define the \Def{ensemble
average} of a function $f(p,q)$ as
\begin{equation}
  \langle f \rangle = \frac{\int f(p, q) \rho(p, q; t)\; \mathrm{d}\omega}{\int \rho(p, q; t)\; \mathrm{d}\omega}.
\end{equation}
The ensemble is said to be \Def{stationary} if $\rho$ is time independent. This
describes a system in equilibrium.

\begin{theorem}[Liouville's theorem]
The local density of points in the phase space $\rho$, as seen by a co-moving
observe along the point's trajectory, stays constant in time, i.e.
\begin{equation*}
  \frac{\mathrm{d}\rho}{\mathrm{d}t} = \frac{\partial \rho}{\partial t} + \{q, H\} = 0,
\end{equation*}
where $\{\cdot,\cdot\}$ is the Poisson bracket. \qedwhite
\end{theorem}

Following from Liouville's theorem, if a system is in equilibrium, $\partial
\rho / \partial t = 0$, so that
\begin{equation*}
  \{q, H\} = \frac{\partial \rho}{\partial q_i} \dot{q}_i + \frac{\partial \rho}{\partial p_i} \dot{p}_i = 0,
\end{equation*}
which greatly reduces the arbitrary nature of $\rho$.

%-------------------------------------------------------------------------------

\section{Microcanonical ensemble}

Mathematically, we assume the density function $\rho(q,p;t)$ is constant in the
accessible regions of the phase space. Physically, we assume we have a
completely isolated system with some Hamiltonian $H$. We want to fix the total
energy to be $E_0$, and allow for some fluctuations of size $\delta E$ around
$E_0$. Since the density function $\rho(p,q;t)$ gives the probability of finding
the representative points at $(q,p)$ in the ensemble, the \Def{microcanonical
ensemble} $\Sigma_{M}$ corresponds to having equal probability of finding any
specific microstate in the energy range $(E_0 \pm \delta E)$.

\begin{example}
  For $\delta E =0 $, we constrain the available region of phase space to the
  hyperspace $H = E_0$, i.e. $\rho(q,p;t) \propto \delta (H(q,p) - E_0)$.
\end{example}

In general, we define
\begin{equation}
  \Omega(E_0, \delta E) = \sum_n \int^{E_0 + \delta E}_{E_0 - \delta E} \delta(E_n - E)\; \mathrm{d}E,
\end{equation}
which is the number of microstates in the energy band $(E_0 \pm \delta E)$.
Here, $E_n$ is the energy of the $n$ microstate, and equal probabilities imply
that the probability of finding a state with energy $E_n$ is
\begin{equation*}
  p_n = \int^{E_0 + \delta E}_{E_0 - \delta E} \frac{\delta (E_n - E)}{\Omega(E_0, \delta E)}\; \mathrm{d}E.
\end{equation*}

Note that $\Sigma_M$ is assumed to be a stationary ensemble, so the ensemble
average must be time-independent. Let $\overline{f}$ denote the ensemble average
of $f$, and $\langle f\rangle_t$ denote the time average of $f$, then
\begin{equation*}
  \overline{f} = \langle \overline{f}\rangle_t = \overline{\langle f\rangle_t} = \langle f\rangle_t,
\end{equation*}
and this is the long time average of $f$ and corresponds to the value one would
obtain on making an appropriate measurement of $f$.

Knowledge of the number of microstates of the system enables us to define the
entropy using the \Def{Boltzmann formula}
\begin{equation}
  S = k_B \log \Omega,
\end{equation}
where $k_B$ is the \Def{Boltzmann constant}\sidenote{$k_B = 1.380649\times
10^{-23}\ \textnormal{J}\ \textnormal{K}^{-1}$}.
To motivate this formula, consider two systems $\mathcal{S}_{1,2}$, each of
which is separately in equilibrium with energies $E_{1,2}$ and number of
microstates as $\Omega_1(E_1)$ and $\Omega_2(E_2)$. Bringing the two systems
together and letting them equilibrate, the total energy $E = E_1 + E_2$ stats
constant and the number of total microstates for the combined system is
\begin{equation*}
  \Omega(E_1, E_2) = \Omega_1 (E_1) \Omega_2 (E_2) = \Omega_1 (E_1) \Omega_2 (E - E_2) \equiv \Omega(E, E_1).
\end{equation*}
We postulate that the energy distribution between the system occurs in such a
way so as to maximise the total number of microstates of the combined system.
Suppose equilibrium values of the energies are labelled as $E_{1, \mathrm{eq}}$
and $E_{2, \mathrm{eq}}$, then, by chain rule and definition of $E_2$,
\begin{align*}
  0 = \left.\frac{\partial \Omega}{\partial E_1}\right|_{E_{1, \mathrm{eq}}} &= \left[\frac{\partial \Omega_1}{\partial E_1} \Omega_2 (E_2)\right]_{E_{1, \mathrm{eq}}} + \left[\frac{\partial \Omega_2}{\partial E_2} \Omega_1 (E_1) \frac{\partial E_2}{\partial E_1}\right]_{E_{2, \mathrm{eq}}} \\
    & = \left[\frac{\partial \Omega_1}{\partial E_1} \Omega_2 (E_2)\right]_{E_{1, \mathrm{eq}}} - \left[\frac{\partial \Omega_2}{\partial E_2} \Omega_1 (E_1)\right]_{E_{2, \mathrm{eq}}},
\end{align*}
and since $\Omega_{1,2} \neq 0$, this implies
\begin{equation*}
  \left.\frac{\partial \Omega_1}{\partial E_1}\right|_{E_{1, \mathrm{eq}}} = \left.\frac{\partial \Omega_2}{\partial E_2}\right|_{E_{2, \mathrm{eq}}}.
\end{equation*}
This means that the condition of equilibrium boils down to the quantity
\begin{equation}
  \beta \equiv \frac{\partial \Omega}{\partial E},
\end{equation}
which is to be constant throughout the system. It is natural to associate
$\beta$ with $(k_B T)^{-1}$, since
\begin{equation*}
  \left.\frac{\partial S}{\partial E}\right|_{V,N} = \frac{1}{T} = k_B \beta.
\end{equation*}

Let the total number of states accessible to a system with energy less than or
equal to $E$ be
\begin{equation*}
  \Sigma (E) = \int_0^E \Omega(E')\; \mathrm{d}E',
\end{equation*}
and let the \Def{density of states} be
\begin{equation}
  D(E) = \frac{\mathrm{d}\Sigma(E)}{\mathrm{d}E}.
\end{equation}

\begin{example}
For an ideal gas, consider $N$ free gas particles of mass $m$ in a volume $V$
with total energy $E$. To calculate the entropy of this system, note that the
canonical Hamiltonian for this system is
\begin{equation*}
  H = \sum_{i=1}^N \frac{|p_i|^2}{2m},
\end{equation*}
implying
\begin{equation*}
  \Sigma(E) = \int_{H \leq E} \frac{\mathrm{d}\omega}{\omega_0},
\end{equation*}
where $\omega_0$ is the volume occupied by a single microstate\sidenote{It turns
out $\omega_0 = h^{3N}$ in 3-space, where $h$ is the Planck's constant.}. We
need to convert the integral over volume in phase space to be specified in terms
of $E$. For this, note that
\begin{equation*}
  \mathrm{d}\omega = \prod^{3N}_{i=1}\ \mathrm{d}q_i\ \mathrm{d}p_i = V^N\ \prod^{3N}_{i=1}\mathrm{d}p_i,
\end{equation*}
\end{example}
so that
\begin{equation*}
  \Sigma(E) = \frac{V^N}{\omega_0} \int_R \prod^{3N}_{i=1}\ \mathrm{d}p_i = \frac{V^N}{\omega_0}\ \mbox{vol}\left(S_r^{3N-1}\right),
\end{equation*}
where $R = \{ p_i | \sum^N_{i=1} |p_i|^2 \leq 2m E\}$, and $S_r^{3N-1}$ is the
$(3N-1)$-sphere of radius $r$. Now, it can be shown that the $(n-1)$-sphere of
unit radius has area
\begin{equation*}
 \mbox{area}\left(S_1^{n-1}\right) = \frac{2\pi^{n/2}}{\Gamma(n/2)}
\end{equation*}
where $\Gamma$ is the gamma-function. So then
\begin{align*}
  \mbox{vol}\left(S_r^{3N-1}\right) = \int_0^r \mbox{area}\left(S_\rho^{n-1}\right)\; \mathrm{d}\rho &= \mbox{area}\left(S_1^{n-1}\right) \int_0^r \rho^{n-1}\; \mathrm{d}\rho \\
    &= \frac{2\pi^{n/2}}{\Gamma(n/2)}\frac{r^n}{n},
\end{align*}
from which it follows that, taking $r = (2mE)^{1/2}$ (because $R$ is a sphere of
radius $(2mE)^{1/2}$
\begin{equation*}
  \Sigma(E) = \frac{V^N}{h^{3N}}\frac{\pi^{3N/2}}{\Gamma(3N/2)}\frac{(2mE)^{3N/2}}{3N/2}.
\end{equation*}
For some small $\delta$ shell, we have the following relation
between $\Sigma(N,V,E)$ and $\Omega(N,V,E,\delta)$:
\begin{equation*}
  \Omega \approx \frac{\partial \Sigma}{\partial E}\delta \approx \frac{3N}{2} \frac{\delta}{E}\Sigma
\end{equation*}
arguing that $3N \approx 3N-1$ for $N \gg 1$. So we have
\begin{equation*}
  \log \Omega \approx \log\Sigma + \log\frac{3N}{2} + \log\frac{\delta}{E}.
\end{equation*}
For $N\gg1$ the last term can be dropped but the ordering between the first and
second term is not obvious. However, using \Def{Sterling's formula} where
\begin{equation}
  \log N! \approx N \log N - N \qquad \textnormal{for}\qquad N\gg1,
\end{equation}
noting that $\Gamma(N) \sim N!$, we have that
\begin{align*}
  \log \Sigma &\approx N\left[\log \left(\frac{2m\pi EV^{2/3}}{h^2}\right)^{3/2} - \frac{3}{2}\log\frac{3N}{2} + \frac{3}{2} + O\left(\frac{\log N}{N}\right)\right] \\
    &= O(N\log N),
\end{align*}
so we have $\log \Omega \sim \log \Sigma$ for $N\gg1$. After tidying up factors,
we obtain \marginnote{Note normally there should be a $1/N!$ factor in the
normalisation to account for the fact that particles are indistinguishable,
which would then modify the resulting formula for the entropy given here to have
the order one constant going from $3/2$ to $5/2$, and $V$ replaced by $V/N$,
resulting in the \Def{Sackur--Tetrode equation}. As given here without the
factor of $1/N!$, $S$ is not extensive nor is it additive, resulting in
\Def{Gibbs' paradox}.}
\begin{equation*}
  S \approx k_B\log \Sigma \approx k_B N \left\{\frac{3}{2} + \log\left[\left(\frac{4\pi mE}{3h^2 N}\right)^{3/2}V\right]\right\}.
\end{equation*}
Compare the derived result here with one obtained from thermodynamics, where
\begin{equation*}
  S \sim S_0 + N k_B\log\left(\frac{u^{3/2}v}{u_0^{3/2}v_0}\right).
\end{equation*}
One can derive the usual thermodynamic equations from $S$, for example,
\begin{equation*}
  \frac{1}{T} = \left.\frac{\partial S}{\partial E}\right|_{V,N} = Nk_B\frac{3}{2}\frac{1}{E} \quad \Rightarrow \quad E = \frac{3Nk_B}{2}T,
\end{equation*}
which agrees with the $u = (3/2)RT$ relation we know from thermodynamics.

%-------------------------------------------------------------------------------

\section{Canonical ensemble}

Physically, it is easier to fix a system's temperature rather than the energy,
as is the case for the microcanonical ensemble. Fixing a system's temperature
can be achieved by coupling the system to a heat bath or reservoir. The
corresponding ensemble is called the \Def{canonical ensemble}, denoted
$\mathcal{E}_c$. In this system, the energy can fluctuate between zero and plus
infinity, and we can only meaningfully take about the probability $p_r$ of
finding a state of energy $E_r$. To determine $p_r$, we can either
\begin{enumerate}
  \item regard the system as being in equilibrium with the reservoir at some
  common temperature $T$, and study the statistics of energy exchange, or
  \item regard the system as a member of an ensemble $\mathcal{E}_c$ at
  temperature $T$,and study the statistics of how the total energy $E$ is shared
  between members of $\mathcal{E}_c$.
\end{enumerate}

Taking the first approach, we consider a system $S$ begin in equilibrium with a
reservoir $R$ at some temperature $T$. We want to find the most likely
distribution of energy $E_S$ of a system and $E_R$ of the reservoir. For
isolated $S\cup R$, the total energy $E = E_S + E_R$ is fixed, and we consider
\begin{equation*}
  \frac{E_S}{E} = 1 - \frac{E_R}{E} \ll 1.
\end{equation*}
For a given state of $S$, $R$ will be in any of the states accessible to it, and
we denote the number of such states $\Omega_R(E_R)$. At equilibrium, we expect
that $\Omega_E(E_R)$ is maximised, and the same is true for the related
quantities of $S$, so
\begin{equation*}
  p_{E_S} \propto \Omega_R(E_R) = \Omega_R(E - E_S).
\end{equation*}
Since $E_S \ll E$, Taylor expanding $\log\Omega_R$ gives
\begin{align*}
  \log \Omega_R(E_R) &= \log\Omega_R(E) - \left.\frac{\partial \log\Omega_R}{\partial E_R}\right|_{E_R = E} E_S + \ldots\\
    &= \log\Omega_R(E) - \beta E_S + \ldots,
\end{align*}
recalling that the entropy is $k_B \log \Omega$ and that $1/T = \partial S /
\partial E = k_B \log \partial \Omega / \partial E$, as well as $\beta = \beta_R
= \beta_S$ since they are thermally coupled and in equilibrium. Then taking an
exponential of both sides and normalising results in
\begin{equation}
  p_{E_S} = \frac{\ex^{-\beta E_S}}{\sum_S \ex^{-\beta E_S}},
\end{equation}
where we are summing over all accessible states in $S$. The factor $\ex^{-\beta
E_S}$ is called the \Def{Boltzmann factor}.

Taking the second approach, we view $S$ as a member of $\mathcal{E}_c$. Assuming
we have $N$ identical systems labelled by $i$ sharing energy $E$, the energy of
a system $r$ as $E_r$, and the number of states having this energy as $n_r$.
Then
\begin{equation*}
  \sum_r n_r = N, \qquad \sum_r E_r n_r = E.
\end{equation*}
For average energy $U$ of each system, we have $E = NU$. We want to find the
distribution of energy into the various systems $\{n_r\}$. The number of
permutations of $\{n_r\}$ is given by
\begin{equation*}
  W(\{n_r\}) = \frac{N!}{\prod_r n_r !}.
\end{equation*}
All possible states of the ensemble, subject to the constraints, are all equally
likely to occur, and so the frequency of occurrence of $\{n_r\}$ is proportional
to $W(\{n_r\})$. The most probable distribution will be the one for which
$W(\{n_r\})$ is maximised, and can be shown to be
\begin{equation*}
  n_r^* = C\ex^{-\beta E_r},
\end{equation*}
which is the same result above after normalising.

We see the average energy $U$ can be expressed as
\begin{equation*}
  U = \langle E_r \rangle = p_r E_r = \frac{\sum_r E_r \mathrm{e}^{-\beta E_r}}{\sum_r \mathrm{e}^{-\beta E_r}} = -\frac{\partial}{\partial \beta} \log \sum_r \mathrm{e}^{-\beta E_r}.
\end{equation*}
Also recall that the Helmholtz free energy is $F(T, X_i) = U - TS$, and that
$\mathrm{d}F = -S\ \mathrm{d}T - P\ \mathrm{d}V + \mu\ \mathrm{d}N$, so that $-S
= \partial F / \partial T$. Then
\begin{equation*}
  U = F + TS = F - T\frac{\partial F}{\partial T} = -T^2 \frac{\partial}{\partial T} \frac{F}{T} = \frac{\partial (F/T)}{\partial (1/T)} = \frac{\partial (\beta F)}{\partial \beta}.
\end{equation*}
Comparing the terms within the derivative, we have
\begin{equation}
  \log \sum_r \mathrm{e}^{-\beta E_r} = -\beta F = -\frac{F}{k_B T},
\end{equation}
which hives a basic relation between the macroscopic thermodynamics and
microscopic statistical mechanics through $F$.

We define the \Def{partition function} $Z$ by
\begin{equation}
  Z(T, V, N) = \sum_r \mathrm{e}^{-\beta E_r},
\end{equation}
so that
\begin{equation}
  F(T, V, N) = -k_B T \log Z.
\end{equation}
The partition function sums over the states of the appropriate energy levels,
and depends on $V$ and $N$. The knowledge of $Z$ completely specified the
thermodynamic properties of the system (since we then have $F$ and thus all the
other thermodynamic quantities).

\begin{example}
  For the entropy $S$, note that
  \begin{align*}
    \left\langle \log p_r \right\rangle &= \left\langle \log \left(\frac{\mathrm{e}^{-\beta E_r}}{Z}\right) \right\rangle = \left\langle -\log Z - \beta E_r \right\rangle\\
      &= -\log Z - \beta \left\langle E_r \right\rangle = \beta F - \beta U\\
      &= -\frac{S}{k_B},
  \end{align*}
  thus $S = -k_B \left\langle \log p_r \right\rangle = -k_B \sum_r p_r \log
  p_r$, as $Z$ and $\beta$ are fixed over the ensemble\marginnote{Note that
  similarities with the \Def{Shannon information entropy} $H = -\sum_r p_r \log
  p_r$.}
\end{example}

The above demonstration implies that the entropy is completely specified by the
probability distribution (given by the Boltzmann weights $\mathrm{e}^{-\beta
E_r}$). If the ground state of the system is unique then $S=0$, as prescribed by
the third law of thermodynamics. Physically this relates the entropy to the
degree of uncertainty of the system, hence entropy is sometimes viewed as a
measure of disorder. Note that for the case where only one $E_r$ is available
(i.e. fixed energy, so like in the micro-canonical ensemble) and $n_r = \Omega$,
\begin{equation*}
  S = -k_B \sum_r p_r \log p_r = -k_B \sum \Omega^{-1} \log \Omega^{-1} = -k_B \log \Omega^{-1} = k_B \log \Omega.
\end{equation*}

Consider a physical system that has some degeneracy where $g-i$ states have
energy $E_i$. Then $Z = \sum_i g_i \mathrm{e}^{-\beta E_i}$, and the probability
of finding a state with energy $E_i$ is (no sum)
\begin{equation*}
  p_i = \frac{1}{Z} g_i \mathrm{e}^{-\beta E_i}.
\end{equation*}
If we can consider the system where the energy spacings are sufficiently fine,
then we can talk about the probability of finding a state with energy in ($E,
E+\delta E$) using the \Def{density of states} $D(E)$, where $D(E)\ \mathrm{d}E$
is the number of states in ($E, E+\delta E$). With that,
\begin{equation*}
  p(E)\ \mathrm{d}E = \frac{\mathrm{e}^{-\beta E} D(E)\ \mathrm{d}E}{\int_0^\infty \mathrm{e}^{-\beta E} D(E)\ \mathrm{d}E} = \frac{\mathrm{e}^{-\beta E} D(E)\ \mathrm{d}E}{Z(T)}
\end{equation*}
where $p(E)$ is the probability of finding the states with energy in the
appropriate energy band. We note that the partition function $Z(T)$ as an
integral is the Laplace transform of $D(E)$ with respect to $\beta$. This can be
formally inverted as
\begin{equation}
  D(E) = \frac{1}{2\pi \mathrm{i}} \int_{\beta' - \mathrm{i}\infty}^{\beta' + \mathrm{i}\infty} Z(\beta) \mathrm{e}^{\beta E}\; \mathrm{d}\beta,
\end{equation}
where $\beta' > 0$, and the integral is taken in the usual sense for Laplace
transform inversions.

%-------------------------------------------------------------------------------

\subsection{Ensemble averages in $\mathcal{E}_c$}

%===============================================================================

\chapter{Quantum statistics}

%===============================================================================



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% r.5 contents
%\tableofcontents

%\listoffigures

%\listoftables

% r.7 dedication
%\cleardoublepage
%~\vfill
%\begin{doublespace}
%\noindent\fontsize{18}{22}\selectfont\itshape
%\nohyphenation
%Dedicated to those who appreciate \LaTeX{} 
%and the work of \mbox{Edward R.~Tufte} 
%and \mbox{Donald E.~Knuth}.
%\end{doublespace}
%\vfill

% r.9 introduction
% \cleardoublepage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% actual useful crap (normal chapters)
\mainmatter

%\part{Basics (?)}


%\backmatter

%\bibliography{refs}
\bibliographystyle{plainnat}

%\printindex

\end{document}

